---
title: "Chapter 4"
author: "Anita Kurm"
date: "March 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#wd
setwd("C:/Users/JARVIS/Desktop/Uni/Semester 4/Computational modelling/Statistical-Rethinking")

#libraries
pacman::p_load(readr,groupdata2,ggplot2,tidyverse,data.table, rethinking)
```

Linear models
Linear regression as a Bayesian procedure: under a probability interpretation, linear regrssion uses a Gaussian distribution to describe model's uncertainty about some measurement of interest

  4.1. Why normal distributions are normal
    4.1.1. by addition
    football pitch experiment:
```{r}
#generate 16 random numbers per person representing individual steps of a random distance between 0 and 1 yard in either of the disrections <- runif(16,-1,1)
#add steps together to get the position after all 16 steps <- sum()
#replicate the procedure for 1000 people <- replicate(1000,...)
pos<- replicate(1000, sum(runif(16,-1,1)))

#plot the distribution
hist(pos)
plot(density(pos))
```
      we can see how normality emerges out of randomness
      -> random values "cancel each other out", i.e. a large positive fluctuation will cancel a large negative one -> convergence (more values in a sum, more values cancel each other out, so the sum tends to end up at zero relative to the mean.)

    4.1.2 by multiplication
    another way to get the normal distribution..
    loci and their growth effects multiply:
```{r}
#sample a random growth rate: sample 12 random numbers between 1.0 (no growth) and 1.1 (10% increase), representing a proportional increase in growth
prod(1+runif(12,0,0.1)) #the product of these 12 numbers is computed 

#generate 10000 random products
growth<- replicate(10000, prod(1+runif(12,0,0.1)))

#plot
dens(growth, norm.comp = T) #overlays normal distribution
```
      small effects that multiply together are approx additive  -> tend to stabilize on Gaussian distribution
```{r}
big<- replicate(10000, prod(1+runif(12,0,1)))
avg<- replicate(10000, prod(1+runif(12,0,0.4)))
small<- replicate(10000, prod(1+runif(12,0,0.01)))

#plot
dens(big, norm.comp = T)
mtext("BIG EFFECTS")
dens(avg, norm.comp = T)
mtext("BIG EFFECTS")
dens(small, norm.comp = T)
mtext("SMALL EFFECTS")
```
      
    With small effects the range of casual forces that tend towards Gaussian distributions extends beyond purely additice interactions
 
    4.1.3 By log-multiplication
    Large deviates (big effects) multiplied together do not produce Gaussian distributions, but they tend to produce Gaussian distributions on the log scale (so do the small deviates)
```{r}
log.big<- replicate(10000, log(prod(1+runif(12,0,1))))
dens(log.big, norm.comp = T)
mtext("BIG EFFECTS ON THE LOG SCALE")

log.small<- replicate(10000, log(prod(1+runif(12,0,0.01))))
dens(log.small, norm.comp = T)
mtext("SMALL EFFECTS ON THE LOG SCALE")
```
    Adding logs is equivalent to multiplying the original numbers ->
        log scales are used to measure sound and earthquakes and information (see chapter 6)

    4.1.4 Using Gaussian distributions
    - a member of exponential familyof fundamental natural distributions
    as a skeleton for hypotheses, building up models of measurements as aggregations of normal distributions. Justified by:
    
        4.1.4.1. Ontological justification
        it's a widespread (but not universal) pattern: measurement errors, variations in growth, velocities of molecules tend towards normal distribution <- cause these processes add together fluctuations(variations) -> results in a distribution of sums that have lost all info aside from mean and spread
        -> gaussian based models cannot reliably identify micro-process, still useful tho
        
         4.1.4.2. Epistemological justification
         it represents a particular state of ignorance: gaussian says only mean and variance of the distribution of measures
         a measure has finite variance <- it's least surprising and least informative assumption <- gaussian does not add any new assumptions <-gaussian is the most consistent with the model's assumptions
         this justification is premised on Inofrmation theory and maximum entropy
         
  4.2 A language for describing models
    1) Recognize outcome variable(s) - a set of measurements that we hope to predict or understand
    2) For each outcome variable, we define a likelihood distribution that defines the plausibility of individual observations <- Gaussian distribution in linear regression
    3) Recognize predictor variables - a set of other measurements we hope to use to predict or understand the outcome
    4) Relate the exact shape (location, variance etc) of the likelihood distribution to the predictor variables <- we are forced to name and define all of the parameters of the model 
    5) Choose priors for all of the parameters in the mode -> priors define the inital information state of the model, before seeing the data
    
    Models as mappings of one set of variables through a probability distribution onto another set of variables 
      4.2.1 Re-describing the globe tossing model
       w ~ Binomial(n,p)   <- the count w is distributed binomially with sample size n and probability p -> binom.distr. assumptions: each sample is independent of the others, sample points are independent of one another
       p ~ Uniform (0,1)   <- the prior p is assumed to be uniform between zero and one
       
       ~ means stochatic relationship (just a mapping of a variable or parameter onto a distribution), where no single instance of the variable on the left is known with certainty -> the mapping is probabilistic (some values are more plausible than others)
       
  4.3. A Gaussian model of height
    regression as soon as we have a predictor c´variable in it
    We want our Bayesian machine to consider every possible distribution (each defined by a combination of mu and sigma) and rank them by posterior plausibility - a measure of the logical compatibility of each possible distribution with the data and model
    -> we'll get an estimate of plausibility of every combination in light of data - a posterior distribution of Gaussian distributions ranked by plausibility
      4.3.1. The data
```{r}
#load the dataframe
data(Howell1)
d<- Howell1

#inspect the structure of tha dataframe
str(d)

#access the vector - column cont. the heights
d$height

#filter out non adults
d2<- d[d$age >= 18,]

dens(d2$height)
```
      4.3.2. The model
      the general model: hi ~ Normal(mu,sigma)    
      
      <- h refers to the list of heights, and the subscript i means each individual element of this list (index) <- the model knows that each height measurement is defined by the same normal distribution, with mean mu and std dev sigma
      the values hi are independent and identically distributed (iid)
      
      To complete the model, we need priors:
      we need a prior Pr(mu,sigma), since both mu and sigma are the parameters to be estimated - this prior is the joint probability for all parameters
      Pr(mu, sigma)= Pr(mu)Pr(sigma)
      
      So our model:
      hi ~ Normal(mu,sigma)     [likelihood]
      mu ~ Normal(178,20)       [mu prior]
      sigma ~ Uniform(0,50)     [sigma prior] flat
       
```{r}
#plot your priors

#prior for the mean - a broad gaussian prior centered on 178 (cause author is that tall, duh) with 95% of probability between 178+-40
curve(dnorm(x,178,20), from=100, to=250)   #the average height is almost certainly between 140 and 220cm - a bit of info from the prior

#prior for the std dev - a flat prior, a uniform one, so std dev has positive probability between 0 and 50cm
curve(dunif(x,0,50), from=-10, to=60)    #a std dev of 50cm implies that 95% ofindividual heights lie within 100cm of the average height

#sample from the prior
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <-runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)


```

      We get the distribution of relative plausibilities of different heights before seeing the data
      
      4.3.3. Grid approximation of the posterior distribution
      let's see how it looks on a model with more than one parameter 
      super computationally expensive, so will use approximations later
```{r}
#a list of possible mu values
mu.list<- seq(from=140, to=160, length.out = 200)

#a list of possible sigma values
sigma.list<- seq(from=4, to=9,length.out = 200)

post<-expand.grid(mu=mu.list, sigma=sigma.list)
post$LL <- sapply(1:nrow(post), function(i) sum( dnorm(
  d2$height,
  mean = post$mu[i],
  sd= post$sigma[i],
  log = TRUE
)))

post$prod <- post$LL+dnorm(post$mu, 178, 20, TRUE)+
  dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))  #posterior distribution

#plot
#contour plot
contour_xyz(post$mu, post$sigma, post$prob)

#a sample heat map
image_xyz(post$mu, post$sigma, post$prob)
```
      4.3.4. Sampling from the posterior
      to study the posterior in more detail..
      Chapter 3: we scoop a bunch of parameter values from the posterior and assume, that they were well mixed -> the samples will have the same proportions as the exact posterior density
      i.e. the individual values of parameters will appear in samples in proportion to the posterior plausibility of each value (we'll have more parameter values from around the  peak, than from the tails)
      Samples are just vectors of numbers, so you can compute any statistic from them that you could from ordinary data
      
      New trick! More than one parameter and we need to sample combinations of paramaters -> first randomly sample row numbers in post in proportion to the values in post$prob  (posterior distribution) -> then pull the parameter values on those randomly sampled rows
      code:
```{r}
#randomly sample row numbers in the dataframe in proportion to the values in the posterior distribution
sample.rows <- sample(1:nrow(post), size = 1e4, replace = TRUE,
                      prob = post$prob)

#pull the parameter values on those randomly sampled rows
sample.mu <-post$mu[sample.rows]
sample.sigma <-post$sigma[sample.rows]

#plot samples
#cex - character expansion/the size of the points
#pch - plot character
#0.1 - transparency value
plot(sample.mu, sample.sigma, cex= 0.6, pch=16, col=col.alpha(rangi2,0.1))
```
      
    By summarizing samples you can describe the distribution of confidence in each combination of mu and sigma  
    
    For instance:
    Characterize the shapes of the marginal (averaging over the other parameters) posterior densities of mu and sigma
```{r}
dens(sample.mu)
mtext("Marginal posterior density of mu, averaging over sigma")
dens(sample.sigma)
mtext("Marginal posterior density of sigma, averaging over mu")

```

    Chapter 3: Highest Posterior Density Interval (HPDI) - narrowest interval containing the specified probability mass - the densest of these intervals
    
```{r}
#summarize the widths of these densities with highest posterior density intervals
#find narrowest region with highest amount of the posterior probability
HPDI(sample.mu)
HPDI(sample.sigma)
```
    
    you can do all kinds of stuff with samples (see chapter 3)
    
    The same code as above, but with less data -> problems with posterior sigma distribution -> non-gaussian if sample is small (has a long right-hand tail, since it's more uncertain about big std deviations than about std dev close to zero)
```{r}
#just less data
d3 <- sample(d2$height, size = 20)

#a list of possible mu values
mu.list<- seq(from=150, to=170, length.out = 200)

#a list of possible sigma values
sigma.list<- seq(from=4, to=20,length.out = 200)

post2<-expand.grid(mu=mu.list, sigma=sigma.list)
post2$LL <- sapply(1:nrow(post2), function(i) sum( dnorm(
  d3,
  mean = post2$mu[i],
  sd= post2$sigma[i],
  log = TRUE
)))

post2$prod <- post2$LL+dnorm(post2$mu, 178, 20, TRUE)+
  dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))  #posterior distribution

#randomly sample row numbers in the dataframe in proportion to the values in the posterior distribution
sample2.rows <- sample(1:nrow(post2), size = 1e4, replace = TRUE,
                      prob = post2$prob)

#pull the parameter values on those randomly sampled rows
sample2.mu <-post2$mu[sample2.rows]
sample2.sigma <-post2$sigma[sample2.rows]

#plot samples
#cex - character expansion/the size of the points
#pch - plot character
#0.1 - transparency value
plot(sample2.mu, sample2.sigma, cex= 0.5, col=col.alpha(rangi2,0.1), xlab="mu", ylab="sigma", pch=16)

#marginal posterior density for sigma, averaging over mu
dens(sample2.sigma, norm.comp = TRUE)
mtext("Marginal posterior density of sigma, averaging over mu")


```
    So, be careful with approximations, if sigma density posterior is important (the distribution is not gaussian, long right-side tail)
    
     4.3.5. Fitting the model with map
      QUADRATIC APPROXIMATION FTW!
      - quickly make inferences about the shape of the posterior
      Posterior's peak lies at the maximum a posteriori estimate (MAP)
      -> use quadratic approx of the posterior distribution at this peak -> get an image of the posterior's shape 
      map command is used to find the values of mu and sigma that maximize the posterior probability
      
      
      Define the model:
      hi <- Normal(mu,sigma)
      mu <- Normal(178,20)
      sigma <- Uniform(0,50)
      
```{r}
#use Howell data again, d2 - only adults
#place R code equivalents of model formulas into an alist
#alist instead of list, so R won't start executing and calculating the objects inside it
flist<- alist(
  height~dnorm(mu,sigma),
  mu~dnorm(178,20),
  sigma~dunif(0,50)
)

#fit the model to the data d2
m4.1 <- map(flist, data=d2)

#look at the fit maximum a posteriori 
precis(m4.1)

#if you want other than default 89% interval..
#precis(m4.1, prob = 0.95)

```
    we see Gaussian approxiamtions for each parameter's marginal distribution
      i.e. the plausibility of each value of mu, after averaging over the plausibilities of each value of sigma, is given by a Gaussian distribution with mean 154.61 and standard deviation 0.4
        89% - default percentile interval containing high-probability range of parameter values. "BECAUSE IT IS PRIME"
        89% boundaries are very similar to HPDIs from grid approx -> the posterior is approx Gaussian 

      Try stronger, more informative priors!
      std dev of mu -> 0.1 (instead of 0, so now it's a very narrow prior)
```{r}
#build it all at once
m4.2<- map(
  alist(
    height~dnorm(mu,sigma),
    mu~dnorm(178,0.1),
    sigma~dunif(0,50)
  ),
  data = d2)

precis(m4.2)
```
      Mu: the prior was very concetrated around 178 -> so mu has hardly moved off the prior
      Sigma: the estimate has changed, even though we haven't change its prior
        <- the model was more certain about mu estimates, as the prior insisted, so the model had to estimate sigma conditional on that fact
        
      4.3.6. Sampling from a map fit
      we know how to get a MAP quadratic approximation of the posterior, but how do we get samples from it?
      
      A quadratic approximation to a posterior distribution with more than one parameter dimenasion is just a multi-dimensional Gaussian distribution
          mu and sigma each contribute one dimension
          
      -> R calculates std deviations for all parameters + the covariances among all pairs of parameters
      
      We can describe:
      One-dimensional Gaussian distribution: mean and std dev (or its square, aka variance)
      Multi-dimensional Gaussian distribution: a list of means and a matrix of variances and covariances  
```{r}
#see the matrix of variances and covariances
vcov(m4.1)
```
      
      VARIANCE-COVARIANCE matrix
      - is the multi-dimensional glue of a qudratic approximation
      - tells how each parameter relates to every other parameter in the posterior dstrb
      - can be factored into two vectors:
          1) a vector of variances for the parameters
          2) a correlation matrix: how chnages in any parameter lead to correlated chenges in others
          
```{r}
#list of variances (squares of std devs from precis output)
diag(vcov(m4.1))

#correlátion matrix
cov2cor(vcov(m4.1))
```
        
      1s indicate parameter's correlation with itself (they should be 1s!!!)
      other entries  are closer to zero ->  learning mu tells us nothing about sigma and likewise learning sigma tells us nothing about mu
      
      We want to get samples from multi-dimensional posterior:
      we sample vectors of values (instead of single values from a simple Gaussian dstrb)
```{r}
#sample vectors from multidimensional distribution
post<- extract.samples(m4.1, n=1e4)
head(post)
```
      each value is a sample from the posterior , so the mean and std dev of each column will be very close to the MAP values:
```{r}
precis(post)

#vs
precis(m4.1) #basically the same

#plot post
plot(post)
```
      
      The samples also preserve the covariance between mu and sigma (doesn't matter here, cause mu and sigma don't covary at all in this model) 
      But!! Once you add a predictor variable to your model, covariance matters a lot!!
      
  
  4.4. Adding a predictor
  
  How do height and weight covary?
```{r}
plot(d2$height~d2$weight)
```
  
      now we want to take Gaussian model and incorporate predictor variables
      
        4.4.1. The linear model strategy
        Make mu (the parameter for the mean of a Gaussian distribution) into a linear function of the predictor variable and other parameters we invent
        i.e. make a linear model
          -> assume that the predictor variable has a perfectly constand and additive relationship to the mean of the outcome -> the model computes the posterior distribution of this constant relationship
          
          every possible combination of the parameter values is considered -> with a linear model, some of the parameters now stand for the strength of the association between the outcome (its mean) and the predictor (its value) -> for each combination, the posterior probability is computed (i.e. a measure of relative plausibility, given the model and data) -> i.e. the posterior distribution ranks the combinations of parameter values by their logical plausibility
       
        -> the posterior distribution provides relative plausibilities of the different possible strengths of association, given the assumptions programmed into the model
        
      The basic Gaussian model:
      h i ~ Normal(mu,sigma)     [likelihood]
      mu ~ Normal(178,20)       [mu prior]
      sigma ~ Uniform(0,50)     [sigma prior] flat
        
      
      now we put "weight" into the height model: as the "x" variable  <- x is gonna stand for d2$weight
      now both x and h are lists of measures of the same length
      we want to know how knowing the values in x can help us describe the values in h
      
      to get x (weight) into the model, we define the mean mu as a function of the values in x:
      hi ~ Normal(mui,sigma)       [likelihood]
      mui = alpha + beta xi        [linear model]
      alpha ~ Normal(178,100)      [alpha prior]
      beta ~ Normal(0,10)          [beta prior]
      sigma ~ Uniform(0,50)        [sigma prior] flat
      
      
  With explanations:
    Likelihood:
      hi ~ Normal(mui,sigma)       [likelihood]  <- 
                                  - mu with index i, since the mean mu depends upon unique predictor values on each row                                     i -> i.e. the mean depends upon the row
    
    Linear model:
      mui = alpha + beta xi        [linear model] <- 
                                    - we don't want to estimate the mean mu anymore, since it is constructed from                                             parameters alpha and beta, and the predictor x; 
                                    - the relationship is not probabilistic, it is determenistic (once we know aplha,                                         beta, x i, we know mu i) -> "=" instead of "~" 
                                    - x i refers to the same individual as the value h i, on the same row
                                    - we invented alpha and beta parameters for manipulating mu, allowing it to vary                                          systematically across individuals in the data
                                    
                                    Bu defining this model you ask:
                                    - What is the expected height, when xi = 0? Answer: the paramater alpha (intercept) 
                                    - What is the change in expected height, when xi changes by 1 unit? Answer: the beta       
     Priors:
      alpha ~ Normal(178,100)      [alpha prior] <-
                                    - what we were used to as mu prior
                                    - windened now since it's common for the intercept to swing a long way from the mu
                                    - the flat prior with a huge std dev will allow the intrcpt to move wherever needed
      beta ~ Normal(0,10)          [beta prior] <-
                                    - puts as much probability below zero, as above
                                    - when beta=0 weight has no relationship to height
                                    - will pull probability mass towards zero, leading to more conservative estimates
                                    - 10 is still large std dev, so make it smaller to shrink the dstrb towards zero
                                    - conservative priors are useful for improving inference
      sigma ~ Uniform(0,50)        [sigma prior] flat
      
      
    
        4.4.2. Fitting the model 
        incorporate the new model for the mean into the model specification inside map
        add new parameters to the start list
        
        The model (again...):
        hi ~ Normal(mui,sigma)       [likelihood]
        mui = alpha + beta xi        [linear model] we'll use "<-" as "=", as used by several Bayesian modelfit engines 
        alpha ~ Normal(156,100)      [alpha prior]
        beta ~ Normal(0,10)          [beta prior]
        sigma ~ Uniform(0,50)        [sigma prior] flat
```{r}
#fit the model
m4.3<- map(
  alist(
    height~dnorm(mu,sigma),
    mu<- a + b*weight, #mu is not a parameter anymore, it is defined by the linear model instead
    a ~ dnorm(156,100), #starts at the overall mean
    b ~ dnorm(0,10),  #conservatively start the slope at zero, i.e. no relatioship btwn predictor and outcome
    sigma~dunif(0,50)
  ),
  data = d2)

```
      
      4.4.3. Interpreting the model fit
      they're hard to understand..
      so we do 1) reading tables and 2) plotting
      plot plot plot the posterior distributions and posterior predictions!!!
      
      Plotting the implications of your estimates will allow you to inquire:
      1) whether or not the model fitting procedure worked correctly
      2) the absolute magnitude, rather than merely relative magnitude, of a relationship btwn outcome and predictor
      3) the uncertainty surrounding an average relationship
      4) the uncertainty surrounding the implied predictions of the model, as these are distinct from mere parameter uncertainty
      
      figures in result sections ftw!
      
      but first.. tables
      
        4.4.3.1. Tables of estimates
        because of covariation among parameters, models in general cannot be understood by tables of estimates
```{r}
#inspect the estimates
precis(m4.3)
```
        1) quadratic approx of alpha:
              alpha is an intercept so 113.89 means that a person of weight 0 should be 114cm tall
              nonsense, value of intercept is often unintepretable without also studying beta values
        2) qudratic approx of beta:
              beta is a slope, so 0.90 means that a person 1 kg heavier is expected to be 0.90cm taller;
              89% of the posterior lies btwn 0.84 and 0.97 -> beta values close to 0 or greatly above 1 are highly incompatible with these data and these model; 
              this estimate indicates strong evidence of a positive relationship btwn height and weight
        3) quadratic approx of sigma
              informs us of the width of the distribution of heights around the mean
              so this estimate tells us that 95% of plausible heights lie within 10 cm (2 sigmas) of the mean height
              there is uncertainty about this, as indicated by 89% percentile interval
              
          We also need variance-covariance matrix to describe the quadratic posterior
          variance is described in the precis output, so we need to see correlations among parameters:
```{r}
#same as cov2cor(vcov(m4.3))
precis(m4.3, corr = TRUE)
```
          Now we see correlations among the parameters 
        Alpha and beta are almost perfectly negatively correlated -> means that two parameters carry the same information (i.e. as you change the slope, the best intercept changes to match it)
        it's harmless now, but we'll try to engineer models to avoid it, when possible
        
        Tricks to do that:
          CENTERING - subtracting the mean of a variable from each value
```{r}
#create a centered version of the weight variable
d2$weight.c <- d2$weight - mean(d2$weight)

#the average value of centered weight variable is nearly zero
mean(d2$weight.c)

#refit the model
m4.4<- map(
  alist(
    height~dnorm(mu,sigma),
    mu<- a + b*weight.c, #mu is not a parameter anymore, it is defined by the linear model instead
    a ~ dnorm(156,100), #starts at the overall mean
    b ~ dnorm(0,10),  #conservatively start the slope at zero, i.e. no relatioship btwn predictor and outcome
    sigma~dunif(0,50)
  ),
  data = d2)

#new estimates
precis(m4.4, corr = TRUE)
```
          Beta and sigma estimates haven't changed (or changed within rounding error)
          The estimate for alpha: still expected value of the outcome variable, when the predictor variable is equal 0
            Now the mean value of the predictor is also zero
         centerring the predictor ->  the expected value of the outcome, when the predictor is at its average value
         i.e. the expected height is estimated to be 154.60, when the weight is at its average value 
         (it's easier to interpret the intercept)
         
         4.4.3.2. Plotting posterior inference against the data
         helps in interpretting the posterior + provides an informal check on model assumptions
         if the model predictions don't fit the data -> model probably did not fit correctly or badly specified
```{r}
#superimpose the MAP values for mean height over the actual data
plot(height ~ weight, data=d2)
abline(a=coef(m4.3)["a"], b=coef(m4.3)["b"]) #the function coef returns a vector of MAP values

```
         The line is defined by slope beta and intercept alpha - just the posterior mean, the most plausible line out of an insane amount of lines considered, we get a sense of the magnitude of the estimated influence of a predictor on an outcome (weight on height)
         
         4.4.3.3. Adding uncertainty around the mean
         we need to communicate uncertainty!
         to get the uncertainty on the plot, we could sample a bunch of lines from the posterior distribution and display those lines on the plot
```{r}
#extract some samples from the model
post <- extract.samples(m4.3)

#inspect the first 5 rows
post[1:5,]
```
         the paired values of a and b define a line
         the average of very many of these lines is the MAP line from before, but scatter around it is meaningful, since it alters our confidence in the relationship between the predictor and the outcome
         
         Model re-estimation with different amount of data:
```{r}
#extract the first 10 cases in d2 and reestimate the model
N <- 10
dN <- d2[1:N,]
mN <-  map(
  alist(
    height~dnorm(mu,sigma),
    mu<- a + b*weight , 
    a ~ dnorm(178,100),
    b ~ dnorm(0,10),  #conservatively start the slope at zero, i.e. no relatioship btwn predictor and outcome
    sigma~dunif(0,50)
  ),
  data = dN)

#plot 20 of these lines, to see what teh uncertainty looks like

#extract 20 samples from the posterior
post <- extract.samples(mN, n=20)

#display raw data and sample size
plot(dN$weight, dN$height,
     xlim = range(d2$weight), ylim = range(d2$height),
     col=rangi2, xlab="weight", ylab="height")
mtext(concat("N = ", N))

#plot the lines: loop over all 20 lines using abline to display each 
for (i in 1:20)
  abline(a=post$a[i], b=post$b[i], col=col.alpha("black",0.3))

```
         change amount of data now:
```{r}
#extract the samples and reestimate the model
N <- 300
dN <- d2[1:N,]
mN <-  map(
  alist(
    height~dnorm(mu,sigma),
    mu<- a + b*weight , 
    a ~ dnorm(178,100),
    b ~ dnorm(0,10),  #conservatively start the slope at zero, i.e. no relatioship btwn predictor and outcome
    sigma~dunif(0,50)
  ),
  data = dN)

#plot 20 of these lines, to see what teh uncertainty looks like

#extract 20 samples from the posterior
post <- extract.samples(mN, n=20)

#display raw data and sample size
plot(dN$weight, dN$height,
     xlim = range(d2$weight), ylim = range(d2$height),
     col=rangi2, xlab="weight", ylab="height")
mtext(concat("N = ", N))

#plot the lines: loop over all 20 lines using abline to display each 
for (i in 1:20)
  abline(a=post$a[i], b=post$b[i], col=col.alpha("black",0.3))
```
         the cloud of regression grew more compact as the sample size increases <- a result of the model growing more confident about the location of the mean
         
         4.4.3.4. Plotting regression intervals and contours
         The cloud of regression lines communicates uncertainty, but it's more common to plot an interval or contour around the MAP line
         
         The interval around regression line: incorporates uncertainty in both the slope beta and intercept alpha at the same time
```{r}
#focus on an individual who weights 50kg (in form of the equation  for mu, xi = 50)
mu_at_50 <- post$a + post$b*50

#plot density
dens(mu_at_50, col=rangi2, lwd=2, xlab="mu|weight=50")
```
        joint alpha and beta went into computing -> the variation across those means incorporates uncertainty in and correlation between both parameters 
        components of mu have distribution -> so does mu (adding Gaussian distribtuions always produces a Gaussian dstr)
        the posterior for mu is a distribution -> you can find intervals for it
```{r}
#find the 89% highest posterior density interval for mu at 50kg
HPDI(mu_at_50, prob = 0.89)
```
        the central 89% of the ways for the model to produce the data (,) place the average height between 158.6cm and 159.6cm, assuming the weight is 50 kg
        
        we want to do the same calculation for every possible x (weight) value on the horizontal axis -> to draw 89% HDPIs around the MAP slope
          <- this is done by link fucntion: take the map model fit, sampple from the posterior dstrb, then compute mu for each case in the data and sample from the posterior dstrb
```{r}
#trying the link fucntion, the default is 1000 rows (samples from the posterior)
mu <- link(m4.3)

#it's a matrix
str(mu)
```
        the output of the link function is a matrix of values of mu:
          each row is a sample from the posterior dstrb
          each column is a case (row) in the actual data
          
        it's basically  a distribution of mu for each individual in the original data
        but we want! a distribution of mu for each unique weight value on the horizontal axis
```{r}
#define sequence of weights to compute predictions for! 
#these values will be on the horizontal axis
weight.seq <- seq(from=25, to = 70, by=1) 

#use link to compute mu
#for each sample from posterior
#and for each weight in weight.seq
mu <- link(m4.3, data = data.frame(weight=weight.seq))
str(mu)

#visualize it
#show the first 100 values in the distribution of mu at each weight value  
#use type="n" to hide raw data
plot(height ~ weight, d2, type="n")

#loop over samples and plot each mu value
for (i in 1:100)
  points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2, 0.1))
```
      we got 46 columns in the matrix, since we fed it 46 different values for weight  
      Plot shows the first 100 values in the distribution of mu (each of these piles of mu values is a Gaussian distribution) at each weight value
      we can see that the amount of uncertainty depends upon the value of weight
```{r}
#summarize the distribution of mu
#compute the mean of each column (dimension "2") of the matrix mu
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)

str(mu.mean)
str(mu.HPDI)

```
      each column is for a different weight value 
```{r}
#plot raw data
#fading out points to make line and interval more visible
plot(height ~ weight, data=d2, col=col.alpha(rangi2,0.5))

#plot the MAP line, aka the mean mu for each weight
lines(weight.seq, mu.mean)

#plot a shaded region for 89% HPDI
shade(mu.HPDI, weight.seq)
```
      The !Kung height data, with 89% HPDI of the mean indicated by the shaded region
      
      
      To summarize the recipe for generating predictions and intervals from the posterior of a fit model:
        1. Use link to generate distributions of posterior values fo mu. The default of the function is to use original data, so feed it a list of new horizontal axis values you want to plot your posterior across
        2. Use summary functions like mean or HPDI or PI to find averages and lowr and upper bounds of mu for each value of the predictor variable
        3. Use plotting functions, like lines and shade to draw the lines and intervals
        or other stuff.. it's up to you, really
      
      
      4.4.3.5. Prediction intervals
      Now let's generate 89% prediction interval for actual heights, not just the average height mu. 
      We'll incorporate the std dev sigma and its uncertainty as well
      Above you used samples from posterior tto visualize the uncertainty of mui, the linear model of the mean
      Actual predictions of heights depend also upon the stochastic (~) definition in the first line
      
      The model (again...):
now we want this -> hi ~ Normal(mui,sigma)       [likelihood]  <- the model expects observed heights to be distributed                                              around mu, not right on top of it. The spread around it is defined by sigma
we did this ->      mui = alpha + beta xi        [linear model] 
                    
                    alpha ~ Normal(156,100)      [alpha prior]
                    beta ~ Normal(0,10)          [beta prior]
                    sigma ~ Uniform(0,50)        [sigma prior] flat <- we need to incorporate this in the predictions
    
    This is how it's done
```{r}
#load the dataframe
data(Howell1)
d<- Howell1


#filter out non adults
d2<- d[d$age >= 18,]

#fit the model
m4.3<- map(
  alist(
    height~dnorm(mu,sigma),
    mu<- a + b*weight, #mu is not a parameter anymore, it is defined by the linear model instead
    a ~ dnorm(156,100), #starts at the overall mean
    b ~ dnorm(0,10),  #conservatively start the slope at zero, i.e. no relatioship btwn predictor and outcome
    sigma~dunif(0,50)
  ),
  data = d2)

#define sequence of weights to compute predictions for! 
#these values will be on the horizontal axis
weight.seq <- seq(from=25, to = 70, by=1) 

#simulate heights, the default number of simulations is 1000, I increased it here to make simulation variance smoother
sim.height <- sim(m4.3, data=list(weight=weight.seq), n =1e4)

#see that it is a matrix, it's a matrix of simulated heights for each defined weight value
str(sim.height)

#summarize the simulated heights
#find  89% posterior prediction interval of observable heights, across the values of weight in weight.seq
height.PI <- apply(sim.height,2, PI, prob=0.89)


#use link to compute mu
#for each sample from posterior
#and for each weight in weight.seq
mu <- link(m4.3, data = data.frame(weight=weight.seq))
#summarize the distribution of mu
#compute the mean of each column (dimension "2") of the matrix mu
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)

#plot raw data
plot(height ~ weight, d2, col= col.alpha(rangi2, 0.5))

#draw MAP line
lines(weight.seq, mu.mean)

#draw HPDI region for line 
shade(mu.HPDI, weight.seq)

#draw PI region for simulated heights
shade(height.PI, weight.seq)

```
      89% prediction interval for height, as a fucntion of weight.
        The solid line is MAP estimate of the mean height at each weight.
        The two shaded regions show different 89% plausible regions. 
            The narrow shaded interval around the line is the distribution of mu (posterior distribution of possible mean values). 
            The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight. 
      
      
      4.5. Polynomial regression
      how to model the outcome as a curved function of a single predictor
      
```{r}
#let's see all the data, not just adults
str(d)

#plot raw data, height against weight
plot(height ~ weight, d, col= col.alpha(rangi2, 0.5))

```
      The relationship is visibly curved -> use polynomial regression
        polynomial means equations for mui that add additional terms with squares, cubes or even higher powers of the predictor variable
        just one predictor - the regression is still bivariate, but there is more parameters in the definition of mui
        
      polynomials are hard to interpret -> in general a bad thing to do
      
      a parabolic model of the mean:
      mui = alpha + beta1*xi + beta2*(x^2)i
      
      alpha - intercept
      beta1 - slope
      beta2 - curvature of the relationship
      
      fitting the model is easy, interpreting it is hard
      To fit the model: standardize the predictor variable  (center the variable -> then divide it by its std dev)
      it leaves the mean at zero but also rescales the range of the data
```{r}
#standardize 
d$weight.s <- ( d$weight - mean(d$weight) )/sd(d$weight)

#plot height on weight.s to verify that no info has been lost 
plot(height ~ weight.s, d, col= col.alpha(rangi2, 0.5))
```
      new variable weight.s has mean zero and std dev 1
      See! just the horizontal axis range changed, the relationship is still the same
      
      To fit the parabolic model, modify the definition of mui:
      
      The parabolic model (with very weak priors):
      hi ~ Normal(mui,sigma)       [likelihood]
      mui = alpha + beta1*xi + beta2*(x^2)i        [parabolic model] 
      alpha ~ Normal(178,100)      [alpha prior]
      beta1 ~ Normal(0,10)          [beta prior]
      beta2 ~ Normal(0,10)
      sigma ~ Uniform(0,50) 
      
      Fitting:
```{r}
#build the square of weight.s as a separate value
d$weight.s2 <- d$weight.s^2

#fit the model
m4.5<- map(
  alist(
    height~dnorm(mu,sigma),
    mu<- a + b1*weight.s + b2*weight.s2, 
    a ~ dnorm(178,100), 
    b1 ~ dnorm(0,10),
    b2 ~ dnorm(0,10),
    sigma ~ dunif(0,50)
  ),
  data = d)

#the summary table
precis(m4.5) #hard to understand what it's saying...

#plot the model fits
#calculate the mean relationship and the 89% intervals of the mean and the predictions
weight.seq <- seq( from=-2.2, to=2, length.out = 30 )
pred_dat <- list(weight.s=weight.seq, weight.s2 = weight.seq^2)

#use link to compute mu for each sample from posterior and for each weight in weight.seq
mu <- link(m4.5, data = pred_dat)
#summarize the distribution of mu
#compute the mean of each column (dimension "2") of the matrix mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob=0.89)

#simulate heights, the default number of simulations is 1000, I increased it here to make simulation variance smoother
sim.height <- sim(m4.5, data=pred_dat, n =1e4)
#summarize the simulated heights
#find  89% posterior prediction interval of observable heights, across the values of weight in weight.seq
height.PI <- apply(sim.height,2, PI, prob=0.89)

####PLOTTING, run all together

#plot raw data
plot(height ~ weight.s, d, col= col.alpha(rangi2, 0.5))
#draw MAP line
lines(weight.seq, mu.mean)
#draw HPDI region for line 
shade(mu.PI, weight.seq)
#draw PI region for simulated heights
shade(height.PI, weight.seq)

```
      A second order polynomial regression - the curve does a much better job of finding a central path through the data
      
      A higher-order polynomial regression, a cubic regression:
      hi ~ Normal(mui,sigma)       
      mui = alpha + beta1*xi + beta2*(x^2)i + beta3*(x^3)i  
      alpha ~ Normal(178,100)      
      beta1 ~ Normal(0,10)         
      beta2 ~ Normal(0,10)
      beta3 ~ Normal(0,10)
      sigma ~ Uniform(0,50)
      
```{r}
#use all data - d
#standardizeweight variable
d$weight.s <- ( d$weight - mean(d$weight) )/sd(d$weight)

#build the square of weight.s as a separate value
d$weight.s2 <- d$weight.s^2

#built the cube of weight.s as a separate value
d$weight.s3 <- d$weight.s^3

#fit the model
m4.6<- map(
  alist(
    height~dnorm(mu,sigma),
    mu<- a + b1*weight.s + b2*weight.s2 + b3*weight.s3, 
    a ~ dnorm(178,100), 
    b1 ~ dnorm(0,10),
    b2 ~ dnorm(0,10),
    b3 ~ dnorm(0,10),
    sigma ~ dunif(0,50)
  ),
  data = d)

#the summary table
precis(m4.6) #hard to understand what it's saying...

#plot the model fits
#calculate the mean relationship and the 89% intervals of the mean and the predictions
weight.seq <- seq( from=-2.2, to=2, length.out = 30 )
pred_dat <- list(weight.s=weight.seq, weight.s2 = weight.seq^2, weight.s3 = weight.seq^3)

#use link to compute mu for each sample from posterior and for each weight in weight.seq
mu <- link(m4.6, data = pred_dat)
#summarize the distribution of mu
#compute the mean of each column (dimension "2") of the matrix mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob=0.89)

#simulate heights, the default number of simulations is 1000, I increased it here to make simulation variance smoother
sim.height <- sim(m4.6, data=pred_dat, n =1e4)
#summarize the simulated heights
#find  89% posterior prediction interval of observable heights, across the values of weight in weight.seq
height.PI <- apply(sim.height,2, PI, prob=0.89)

####PLOTTING, run all together

#plot raw data
plot(height ~ weight.s, d, col= col.alpha(rangi2, 0.5))
#draw MAP line
lines(weight.seq, mu.mean)
#draw HPDI region for line 
shade(mu.PI, weight.seq)
#draw PI region for simulated heights
shade(height.PI, weight.seq)
```
      Better fit does not mean a better model
      The solid line is MAP estimate of the mean height at each weight.
        The two shaded regions show different 89% plausible regions. 
            The narrow shaded interval around the line is the distribution of mu (posterior distribution of possible mean values). 
            The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight. 
      
      
      
      Summary:
      Simple linear regression model - a framework for estimating the association between a predictor and an outcome
      The Gaussian distribution comprises (puts together in the same place) the likelihood in such models:
        it counts up relative numbers of ways  different combinations of means and std devs can produce an observation
      
      MAP (maximum a prior) estimation fits these models to data
      Posterior distributions and posterior predictions vizualization 
      
      
      Reminder. 
      Posterior is always a probability distribution -> The posterior defines the expected frequency that different parameter values will apear
      
      