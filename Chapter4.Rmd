---
title: "Chapter 4"
author: "Anita Kurm"
date: "March 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#wd
setwd("C:/Users/JARVIS/Desktop/Uni/Semester 4/Computational modelling/Statistical-Rethinking")

#libraries
pacman::p_load(readr,groupdata2,ggplot2,tidyverse,data.table, rethinking)
```

Linear models
Linear regression as a Bayesian procedure: under a probability interpretation, linear regrssion uses a Gaussian distribution to describe model's uncertainty about some measurement of interest

  4.1. Why normal distributions are normal
    4.1.1. by addition
    football pitch experiment:
```{r}
#generate 16 random numbers per person representing individual steps of a random distance between 0 and 1 yard in either of the disrections <- runif(16,-1,1)
#add steps together to get the position after all 16 steps <- sum()
#replicate the procedure for 1000 people <- replicate(1000,...)
pos<- replicate(1000, sum(runif(16,-1,1)))

#plot the distribution
hist(pos)
plot(density(pos))
```
      we can see how normality emerges out of randomness
      -> random values "cancel each other out", i.e. a large positive fluctuation will cancel a large negative one -> convergence (more values in a sum, more values cancel each other out, so the sum tends to end up at zero relative to the mean.)

    4.1.2 by multiplication
    another way to get the normal distribution..
    loci and their growth effects multiply:
```{r}
#sample a random growth rate: sample 12 random numbers between 1.0 (no growth) and 1.1 (10% increase), representing a proportional increase in growth
prod(1+runif(12,0,0.1)) #the product of these 12 numbers is computed 

#generate 10000 random products
growth<- replicate(10000, prod(1+runif(12,0,0.1)))

#plot
dens(growth, norm.comp = T) #overlays normal distribution
```
      small effects that multiply together are approx additive  -> tend to stabilize on Gaussian distribution
```{r}
big<- replicate(10000, prod(1+runif(12,0,1)))
avg<- replicate(10000, prod(1+runif(12,0,0.4)))
small<- replicate(10000, prod(1+runif(12,0,0.01)))

#plot
dens(big, norm.comp = T)
mtext("BIG EFFECTS")
dens(avg, norm.comp = T)
mtext("BIG EFFECTS")
dens(small, norm.comp = T)
mtext("SMALL EFFECTS")
```
      
    With small effects the range of casual forces that tend towards Gaussian distributions extends beyond purely additice interactions
 
    4.1.3 By log-multiplication
    Large deviates (big effects) multiplied together do not produce Gaussian distributions, but they tend to produce Gaussian distributions on the log scale (so do the small deviates)
```{r}
log.big<- replicate(10000, log(prod(1+runif(12,0,1))))
dens(log.big, norm.comp = T)
mtext("BIG EFFECTS ON THE LOG SCALE")

log.small<- replicate(10000, log(prod(1+runif(12,0,0.01))))
dens(log.small, norm.comp = T)
mtext("SMALL EFFECTS ON THE LOG SCALE")
```
    Adding logs is equivalent to multiplying the original numbers ->
        log scales are used to measure sound and earthquakes and information (see chapter 6)

    4.1.4 Using Gaussian distributions
    - a member of exponential familyof fundamental natural distributions
    as a skeleton for hypotheses, building up models of measurements as aggregations of normal distributions. Justified by:
    
        4.1.4.1. Ontological justification
        it's a widespread (but not universal) pattern: measurement errors, variations in growth, velocities of molecules tend towards normal distribution <- cause these processes add together fluctuations(variations) -> results in a distribution of sums that have lost all info aside from mean and spread
        -> gaussian based models cannot reliably identify micro-process, still useful tho
        
         4.1.4.2. Epistemological justification
         it represents a particular state of ignorance: gaussian says only mean and variance of the distribution of measures
         a measure has finite variance <- it's least surprising and least informative assumption <- gaussian does not add any new assumptions <-gaussian is the most consistent with the model's assumptions
         this justification is premised on Inofrmation theory and maximum entropy
         
  4.2 A language for describing models
    1) Recognize outcome variable(s) - a set of measurements that we hope to predict or understand
    2) For each outcome variable, we define a likelihood distribution that defines the plausibility of individual observations <- Gaussian distribution in linear regression
    3) Recognize predictor variables - a set of other measurements we hope to use to predict or understand the outcome
    4) Relate the exact shape (location, variance etc) of the likelihood distribution to the predictor variables <- we are forced to name and define all of the parameters of the model 
    5) Choose priors for all of the parameters in the mode -> priors define the inital information state of the model, before seeing the data
    
    Models as mappings of one set of variables through a probability distribution onto another set of variables 
      4.2.1 Re-describing the globe tossing model
       w ~ Binomial(n,p)   <- the count w is distributed binomially with sample size n and probability p -> binom.distr. assumptions: each sample is independent of the others, sample points are independent of one another
       p ~ Uniform (0,1)   <- the prior p is assumed to be uniform between zero and one
       
       ~ means stochatic relationship (just a mapping of a variable or parameter onto a distribution), where no single instance of the variable on the left is known with certainty -> the mapping is probabilistic (some values are more plausible than others)
       
  4.3. A Gaussian model of height
    regression as soon as we have a predictor cÂ´variable in it
    We want our Bayesian machine to consider every possible distribution (each defined by a combination of mu and sigma) and rank them by posterior plausibility - a measure of the logical compatibility of each possible distribution with the data and model
    -> we'll get an estimate of plausibility of every combination in light of data - a posterior distribution of Gaussian distributions ranked by plausibility
      4.3.1. The data
```{r}
#load the dataframe
data(Howell1)
d<- Howell1

#inspect the structure of tha dataframe
str(d)

#access the vector - column cont. the heights
d$height

#filter out non adults
d2<- d[d$age >= 18,]

dens(d2$height)
```
      4.3.2. The model
      the general model: hi ~ Normal(mu,sigma)    
      
      <- h refers to the list of heights, and the subscript i means each individual element of this list (index) <- the model knows that each height measurement is defined by the same normal distribution, with mean mu and std dev sigma
      the values hi are independent and identically distributed (iid)
      
      To complete the model, we need priors:
      we need a prior Pr(mu,sigma), since both mu and sigma are the parameters to be estimated - this prior is the joint probability for all parameters
      Pr(mu, sigma)= Pr(mu)Pr(sigma)
      
      So our model:
      hi ~ Normal(mu,sigma)     [likelihood]
      mu ~ Normal(178,20)       [mu prior]
      sigma ~ Uniform(0,50)     [sigma prior] flat
       
```{r}
#plot your priors

#prior for the mean - a broad gaussian prior centered on 178 (cause author is that tall, duh) with 95% of probability between 178+-40
curve(dnorm(x,178,20), from=100, to=250)   #the average height is almost certainly between 140 and 220cm - a bit of info from the prior

#prior for the std dev - a flat prior, a uniform one, so std dev has positive probability between 0 and 50cm
curve(dunif(x,0,50), from=-10, to=60)    #a std dev of 50cm implies that 95% ofindividual heights lie within 100cm of the average height

#sample from the prior
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <-runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)


```

      We get the distribution of relative plausibilities of different heights before seeing the data
      
      4.3.3. Grid approximation of the posterior distribution
      let's see how it looks on a model with more than one parameter 
      super computationally expensive, so will use approximations later
```{r}
#a list of possible mu values
mu.list<- seq(from=140, to=160, length.out = 200)

#a list of possible sigma values
sigma.list<- seq(from=4, to=9,length.out = 200)

post<-expand.grid(mu=mu.list, sigma=sigma.list)
post$LL <- sapply(1:nrow(post), function(i) sum( dnorm(
  d2$height,
  mean = post$mu[i],
  sd= post$sigma[i],
  log = TRUE
)))

post$prod <- post$LL+dnorm(post$mu, 178, 20, TRUE)+
  dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))  #posterior distribution

#plot
#contour plot
contour_xyz(post$mu, post$sigma, post$prob)

#a sample heat map
image_xyz(post$mu, post$sigma, post$prob)
```
      4.3.4. Sampling from the posterior
      to study the posterior in more detail..
      Chapter 3: we scoop a bunch of parameter values from the posterior and assume, that they were well mixed -> the samples will have the same proportions as the exact posterior density
      i.e. the individual values of parameters will appear in samples in proportion to the posterior plausibility of each value (we'll have more parameter values from around the  peak, than from the tails)
      Samples are just vectors of numbers, so you can compute any statistic from them that you could from ordinary data
      
      New trick! More than one parameter and we need to sample combinations of paramaters -> first randomly sample row numbers in post in proportion to the values in post$prob  (posterior distribution) -> then pull the parameter values on those randomly sampled rows
      code:
```{r}
#randomly sample row numbers in the dataframe in proportion to the values in the posterior distribution
sample.rows <- sample(1:nrow(post), size = 1e4, replace = TRUE,
                      prob = post$prob)

#pull the parameter values on those randomly sampled rows
sample.mu <-post$mu[sample.rows]
sample.sigma <-post$sigma[sample.rows]

#plot samples
#cex - character expansion/the size of the points
#pch - plot character
#0.1 - transparency value
plot(sample.mu, sample.sigma, cex= 0.6, pch=16, col=col.alpha(rangi2,0.1))
```
      
    By summarizing samples you can describe the distribution of confidence in each combination of mu and sigma  
    
    For instance:
    Characterize the shapes of the marginal (averaging over the other parameters) posterior densities of mu and sigma
```{r}
dens(sample.mu)
mtext("Marginal posterior density of mu, averaging over sigma")
dens(sample.sigma)
mtext("Marginal posterior density of sigma, averaging over mu")

```

    Chapter 3: Highest Posterior Density Interval (HPDI) - narrowest interval containing the specified probability mass - the densest of these intervals
    
```{r}
#summarize the widths of these densities with highest posterior density intervals
#find narrowest region with highest amount of the posterior probability
HPDI(sample.mu)
HPDI(sample.sigma)
```
    
    you can do all kinds of stuff with samples (see chapter 3)
    
    The same code as above, but with less data -> problems with posterior sigma distribution -> non-gaussian if sample is small (has a long right-hand tail, since it's more uncertain about big std deviations than about std dev close to zero)
```{r}
#just less data
d3 <- sample(d2$height, size = 20)

#a list of possible mu values
mu.list<- seq(from=150, to=170, length.out = 200)

#a list of possible sigma values
sigma.list<- seq(from=4, to=20,length.out = 200)

post2<-expand.grid(mu=mu.list, sigma=sigma.list)
post2$LL <- sapply(1:nrow(post2), function(i) sum( dnorm(
  d3,
  mean = post2$mu[i],
  sd= post2$sigma[i],
  log = TRUE
)))

post2$prod <- post2$LL+dnorm(post2$mu, 178, 20, TRUE)+
  dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))  #posterior distribution

#randomly sample row numbers in the dataframe in proportion to the values in the posterior distribution
sample2.rows <- sample(1:nrow(post2), size = 1e4, replace = TRUE,
                      prob = post2$prob)

#pull the parameter values on those randomly sampled rows
sample2.mu <-post2$mu[sample2.rows]
sample2.sigma <-post2$sigma[sample2.rows]

#plot samples
#cex - character expansion/the size of the points
#pch - plot character
#0.1 - transparency value
plot(sample2.mu, sample2.sigma, cex= 0.5, col=col.alpha(rangi2,0.1), xlab="mu", ylab="sigma", pch=16)

#marginal posterior density for sigma, averaging over mu
dens(sample2.sigma, norm.comp = TRUE)
mtext("Marginal posterior density of sigma, averaging over mu")


```
    So, be careful with approximations, if sigma density posterior is important (the distribution is not gaussian, long right-side tail)
    
     4.3.5. Fitting the model with map
      QUADRATIC APPROXIMATION FTW!
      - quickly make inferences about the shape of the posterior
      Posterior's peak lies at the maximum a posteriori estimate (MAP)
      -> use quadratic approx of the posterior distribution at this peak -> get an image of the posterior's shape 
      map command is used to find the values of mu and sigma that maximize the posterior probability
      
      
      Define the model:
      hi <- Normal(mu,sigma)
      mu <- Normal(178,20)
      sigma <- Uniform(0,50)
      
```{r}
#use Howell data again, d2 - only adults
#place R code equivalents of model formulas into an alist
#alist instead of list, so R won't start executing and calculating the objects inside it
flist<- alist(
  height~dnorm(mu,sigma),
  mu~dnorm(178,20),
  sigma~dunif(0,50)
)

#fit the model to the data d2
m4.1 <- map(flist, data=d2)

#look at the fit maximum a posteriori 
precis(m4.1)

#if you want other than default 89% interval..
#precis(m4.1, prob = 0.95)

```
    we see Gaussian approxiamtions for each parameter's marginal distribution
      i.e. the plausibility of each value of mu, after averaging over the plausibilities of each value of sigma, is given by a Gaussian distribution with mean 154.61 and standard deviation 0.4
        89% - default percentile interval containing high-probability range of parameter values. "BECAUSE IT IS PRIME"
        89% boundaries are very similar to HPDIs from grid approx -> the posterior is approx Gaussian 

      Try stronger, more informative priors!
      std dev of mu -> 0.1 (instead of 0, so now it's a very narrow prior)
```{r}
#build it all at once
m4.2<- map(
  alist(
    height~dnorm(mu,sigma),
    mu~dnorm(178,0.1),
    sigma~dunif(0,50)
  ),
  data = d2)

precis(m4.2)
```
      Mu: the prior was very concetrated around 178 -> so mu has hardly moved off the prior
      Sigma: the estimate has changed, even though we haven't change its prior
        <- the model was more certain about mu estimates, as the prior insisted, so the model had to estimate sigma conditional on that fact
        
      4.3.6. Sampling from a map fit
      we know how to get a MAP quadratic approximation of the posterior, but how do we get samples from it?
      
      A quadratic approximation to a posterior distribution with more than one parameter dimenasion is just a multi-dimensional Gaussian distribution
          mu and sigma each contribute one dimension
          
      -> R calculates std deviations for all parameters + the covariances among all pairs of parameters
      
      We can describe:
      One-dimensional Gaussian distribution: mean and std dev (or its square, aka variance)
      Multi-dimensional Gaussian distribution: a list of means and a matrix of variances and covariances  
```{r}
#see the matrix of variances and covariances
vcov(m4.1)
```
      
      VARIANCE-COVARIANCE matrix
      - is the multi-dimensional glue of a qudratic approximation
      - tells how each parameter relates to every other parameter in the posterior dstrb
      - can be factored into two vectors:
          1) a vector of variances for the parameters
          2) a correlation matrix: how chnages in any parameter lead to correlated chenges in others
          
```{r}
#list of variances (squares of std devs from precis output)
diag(vcov(m4.1))

#correlÃ¡tion matrix
cov2cor(vcov(m4.1))
```
        
      1s indicate parameter's correlation with itself (they should be 1s!!!)
      other entries  are closer to zero ->  learning mu tells us nothing about sigma and likewise learning sigma tells us nothing about mu
      
      We want to get samples from multi-dimensional posterior:
      we sample vectors of values (instead of single values from a simple Gaussian dstrb)
```{r}
#sample vectors from multidimensional distribution
post<- extract.samples(m4.1, n=1e4)
head(post)
```
      each value is a sample from the posterior , so the mean and std dev of each column will be very close to the MAP values:
```{r}
precis(post)

#vs
precis(m4.1) #basically the same

#plot post
plot(post)
```
      
      The samples also preserve the covariance between mu and sigma (doesn't matter here, cause mu and sigma don't covary at all in this model) 
      But!! Once you add a predictor variable to your model, covariance matters a lot!!
      
  
  4.4. Adding a predictor
  
  How do height and weight covary?
```{r}
plot(d2$height~d2$weight)
```
  
      now we want to take Gaussian model and incorporate predictor variables
      
        4.4.1. The linear model strategy
        Make mu (the parameter for the mean of a Gaussian distribution) into a linear function of the predictor variable and other parameters we invent
        i.e. make a linear model
          -> assume that the predictor variable has a perfectly constand and additive relationship to the mean of the outcome -> the model computes the posterior distribution of this constant relationship
          
          every possible combination of the parameter values is considered -> with a linear model, some of the parameters now stand for the strength of the association between the outcome (its mean) and the predictor (its value) -> for each combination, the psoterior probability 
        