---
title: "Chapter 3"
author: "Anita Kurm"
date: "February 17, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#wd
setwd("C:/Users/JARVIS/Desktop/Uni/Semester 4/Computational modelling/Statistical-Rethinking")

#libraries
pacman::p_load(readr,groupdata2,ggplot2,tidyverse,data.table, rethinking)
```

Bayes' Theorem on a vampire example: 
  probability of one being a vampire given positive test for vampirism equals (Probability of a positive result given that the one is actually a vampire multiplied by probability of encountering a vampire in the entire population) divided by the average probability of a positive result  

    the average probability of a positive test result is calculated by:
      (probabilty of a positive result given that one is a vampire multiplied by probability of encountering a vampire in the population) + (probability of a positive result given that one is mortal multiplied by the probability of encountering mortals in the population)
      i.e. probability of true positives + probability of false positives

                       Pr(positive|vampire)*Pr(vampire)    Pr(positive|vampire)*Pr(vampire)
Pr(vampire|positive) = -------------------------------- = -------------------------------- 
                               Pr(positive)               Pr(positive|vampire)*Pr(vampire)+ 
                                                          Pr(positive|mortal)*Pr(mortal)
                                                          
                                                          
                                                          Pr(mortal)= 1-Pr(vampire) duh... 
                                                          
```{r}
PrPV<-0.95 #test correctly detects vampirism 95% of the time
PrPM<-0.01 #false positives, 1% of the time it diagnoses normal people with vampirism
PrV<-0.001 #vampires are rare,only 0.001 of population
PrP<-PrPV*PrV+PrPM*(1-PrV)

(PrVP<-PrPV*PrV/PrP)
```

frequency format / natural frequencies:
1) population=100 000 -> 100 are vampires
2) 95 out of 100 vampires will test positive (that's the accuracy of the test)
3) 999 out of the 99,900 mortals will give false positives

                            number of vampires testing positive
-> Pr(vampire|positive)= ---------------------------------------
                           total number of positives (mortals+vampires)
```{r}
#Performing calculation
95/(999+95)
```

The results is the same.


Posterior is always a probability distribution -> The posterior defines the expected frequency that different parameter values will apear
 -> we can draw samples from it
    the sampled events in this case are parameter values
                
                Reminder.
                Parameters: 
                your Bayesian analysis describes what the data tells about the 
                unknown parameter(s)
                       they may be quantitites that we wish to estimate
                       they represent the different conjectures for causes and explanations 
                       of the data 
                       different inputs of the likelihood fucntion may be the targets of 
                       the analysis 
                
                Posterior: the relative plausibility of different parameter values, 
                conditional on the data
                      the parameter values near the peak are much more common (and likely 
                      to produce the data) than parameter values in the tail
    
We shall learn basic skills for working with samples (parameter values) from the posterior distribution 


1. Sampling from a grid-approximate posterior
    we scoop a bunch of parameter values from the posterior and assume, that they were well
    mixed -> the samples will have the same proportions as the exact posterior density
    i.e. the individual values of parameters will appear in samples in proportion to the 
    posterior plausibility of each value (we'll have more parameter values from around the 
    peak, than from the tails)
```{r}
#Computing the posterior for the model, using grid approximation (globe tossing model again)

#define grid
p_grid<- seq(from=0, to=1, length.out=1000)

#define prior, flat prior in this case
prior<- rep(1,1000)

#compute likelihood at each value in grid
likelihood<- dbinom(6, size = 9, prob = p_grid)

#compute product of likelihood and prior
unstd.posterior<- likelihood*prior

#standardize the posterior, so it sums to 1
posterior<- unstd.posterior/sum(unstd.posterior)

#draw 10 000 samples from the posterior, (10 000= 1e4) 
samples<- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE) 
    #sample function randomly pulls values from the vector p_grid
    #the probability of each value is given by posterior 

#plot samples
plot(samples)
```
      Way more values around 0.6,than around 0.2 or 0.9

```{r}
#show density estimate
dens(samples)
```
      The estimated density is very similar to the ideal posterior from Chapter 2.
      Drawing more samples -> estimated density is more and more similar to the ideal            posterior
      example:
```{r}
#define grid
p_grid<- seq(from=0, to=1, length.out=1000)

#define prior, flat prior in this case
prior<- rep(1,1000)

#compute likelihood at each value in grid
likelihood<- dbinom(6, size = 9, prob = p_grid)

#compute product of likelihood and prior
unstd.posterior<- likelihood*prior

#standardize the posterior, so it sums to 1
posterior<- unstd.posterior/sum(unstd.posterior)

#draw 10 000 samples from the posterior,(1 000 000= 1e6) read 1 times 10 to the power of 6 
samples<- sample(p_grid, prob = posterior, size = 1e6, replace = TRUE) 
    #sample function randomly pulls values from the vector p_grid
    #the probability of each value is given by posterior 

#plot samples
plot(samples)
#show density estimate
dens(samples)
```
        see!!


2. Sampling to summarize
    model's role is to produce a posterior distribution, now you have to summarize and  
    interpret the posterior distribution
    
    Common questions:
      How much posterior probability lies below some parameter value?
      How much posterior probability les between two parameter values?
      Which parameter value marks the lower 5% of the posterior probability?
      Which range of parameter values contains 90% of the posterior probability?
      Which parameter value has highest posterior probability?
      
   2.1. Intervals of defined boundaries
```{r}
#e.g. add up posterior probability where p<0.5
sum(posterior[p_grid<0.5])

```
    About 17% of the posterior probability is below 0.5, how do you do that with samples?
    generalized method, so you can use it anywhere
    sum the samples, that meet the requirement and divide the resulting count by the total 
    number of samples
    
```{r}
sum(samples<0.5)/1e6
```
      nearly the sane answer
    
      Using the same approach: how much posterior probabilty lies between 0.5 and 0.75
```{r}
sum(samples>0.5 & samples<0.75)/1e6
```
      Around 60% of the posterior probability lies between 0.5 and 0.75
   
   2.2. Intervals of defined mass
      e.g.common interval to be reported is a confidence interval
      interval of posterior probability - credible interval
      these posterior intervals report two parameter values that contain between them a 
      probability mass (a specified amount of posterior probability)
      it's mostly easier to use samples, than grid approximation
      
      e.g. percentile intervals (PI)
      find the boundaries of the lower 80% of posterior prob. starts at p=0, so we are
      looking for 80th percentile
```{r}
quantile(samples, 0.8)
```
        or middle 80% (i.e. it's between 10th and 90th percentile)
```{r}
quantile(samples, c(0.1, 0.9))
```
      This approach works well only with symmetrical distriburions
      
      For assymetrical posteriors:
```{r}
#first, create assymetrical posterior
#define grid
p_grid<- seq(from=0, to=1, length.out=1000)

#define prior, flat prior in this case
prior<- rep(1,1000)

#compute likelihood at each value in grid, 3 waters in 3 tosses -> max value at p=1
likelihood<- dbinom(3, size = 3, prob = p_grid)

#compute product of likelihood and prior
unstd.posterior<- likelihood*prior

#standardize the posterior, so it sums to 1
posterior<- unstd.posterior/sum(unstd.posterior)

plot(p_grid, posterior, type="b", xlab="probability of water", ylab="posterior probability")

#draw 10 000 samples from the posterior, (10 000= 1e4) 
samples<- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE) 
    #sample function randomly pulls values from the vector p_grid
    #the probability of each value is given by posterior 

```
```{r}
#50% percentile confidence interval
PI(samples, prob = 0.5)
```
      this interval assigns 25% pf the probability below and above the interval, i.e. 50% 
      probability <- it excludes the most probable parameter values, not good.
      
      Solution: Highest Posterior Density Interval (HPDI) - narrowest interval containing 
      the specified probability mass - the densest of these intervals
```{r}
#find narrowest region with 50% of the posterior probability
HPDI(samples, prob = 0.5) 
```
      HDPI has advantages over PI, especially if the posterior is skewed
      but also has disadvantages - is more computationally intensve,is sensitive to how many samples were drawn ("suffers from simulation variance")
      
      If the choice of interval type makes a big difference, don't use intervals to summarize the posterior.
      The entire posterior distribution is the Bayesian estimate! - it summarizes the relative plausibilities of each possible parameter value. If choice of interval makes big difference, just plot  the entire posterior distribution
      
      
      
   2.3. Point estimates
   - you don't have to... it's hardly ever necessary
   but... you can report 
      a) a maximum a posteriori (MAP) estimate (the mode)
```{r}
#from the grid
p_grid[which.max(posterior)] #it's 1

#from the samples
chainmode(samples, adj=0.01) #0.997
```
      b)mean 
      c)median
```{r}
mean(samples) #0.801
median(samples) #0.843
```
    Or you can use the loss fucntion - tells you the cost associated with using any particular point estimate
      different loss functions imply different point estimates
      
    we are trying to calculate expected loss for any given descision 
```{r}
#we decide/guess that p=0.5 (the proportion of water on the Earth)
#we compute the weighted average loss, where each loss is weighted by its corresponding posterior probability
sum(posterior*abs(0.5-p_grid)) #the loss is 0.31287

#repeat calculation for every possible  and get a list of loss values
loss<- sapply(p_grid, function(d) sum(posterior*abs(d-p_grid)))

#find the parameter value that minimizes the loss
p_grid[which.min(loss)]
```
    THIS IS BASICALLY THE MEDIAN - value that splits the posterior density, so half of the mass is above at and anoither half is below it
```{r}
median(samples)
```
    This was the absolute loss function (d-p) i.e. decision-correct value -> leads to the median as the point estimate 
    There is also the quadratic loss function (d-p)^2 -> leads to the posterior mean as the point estimate
```{r}
mean(samples)
```

3. Sampling to simulate prediction
  Samples ease simulation of the model's implied observations, that are useful for:
      a)Model checking 
      b)Software validation
      c)Research design
      d)Forecasting
      
   3.1. Dummy data
      Bayesian models are always generative, capable of simulating prediction
      Simulated data -> dummy data
      With the globe tossing model, the dummy data arises from a binomial likelihood:
      
                  n!     
  Pr(w|n,p) = ----------p^w(1-p)^(n-w) 
              w!(n-w)!                
```{r}
#suppose n=2 tosses of the globe -> there might be 0 w, 1 w or 2 w
#you can compute the likelihood of each possibility, for any p
#let's use p=0.7
dbinom(0:2,size =2,prob = 0.7 )
```
     9% chance to see 0 observation of water, 42% chnace to see 1 w, 49% chance to see 2 w
     
     you'll get a different distribution of implied observations for different p values
```{r}
dbinom(0:2,size =2,prob = 0.2 )
dbinom(0:2,size =2,prob = 0.5 )
dbinom(0:2,size =2,prob = 0.99 )
```
     
     now we'll try to simulate observations, using these likelihoods, by sampling!
```{r}
#sampling a single dummy data observation of w
rbinom(1,size = 2, prob = 0.7) #r stands for random

#generating 10 simulations 
rbinom(10,size = 2, prob = 0.7)

#generating 100,000 dummy observations, to verify that each value appears in proportion to it's likelihood
dummy_w<- rbinom(1e5,size = 2, prob = 0.7)
table(dummy_w)/1e5
```
     
     the same procedure with 9 tosses
```{r}
dummy_w<-rbinom(1e5,size = 9, prob = 0.7)
simplehist(dummy_w,xlab = "dummy water count")
```
     the most of the time the expected observation does not contain water in its true proportion 0.7 <- due to the nature of observation: one-to-many relationship between data and data-generating processes
     
     experimenting with the sample size:
```{r}
dummy_w<-rbinom(1e5,size = 100, prob = 0.7)
simplehist(dummy_w,xlab = "dummy water count")

dummy_w<-rbinom(1e5,size = 1e5, prob = 0.7)
simplehist(dummy_w,xlab = "dummy water count")
```
     Now we want to use simulated observations in examining the implied predictions of a model.
     So, we need to combine them with samples from the posterior distribution.
                                                          
   3.2. Model checking
        1) ensuring the model fitting worked correctly
        2)evaluating the adequacy of a model for some purpose.
        
        Bayesian models are generative <- once you condition a model on data, you can 
        simulate to examine the model's empirical expectations
        
        You can't really check if your software worked correctly, but you can check how 
        well your model reproduces the data used to educate it (the exact much is neither 
        expected nor desired, but should be some correspondance)
   
      Model accuracy check.
        The goal is to assess exactly how the model fails to describe the data, as a path 
        towards model comprehension, revision, and improvement
        
        We want to learn to combine sampling of simulated observation with sampling from 
        the posterior distribution <- we expect to do better using the entire distribution
         <- cause there is a lot of information about uncertainty, we don't want to lose it
         <- to avoid overconfidence
          
          so we try to propagate the parameter uncertainty embedded in the posterior   
          distribution, as we evaluate the implied predictions
          all that is required is averaging over the posterior density for p, while 
          computing the predictions
          -> for each value of p, there is an implied distribution of outcomes
          -> if you compute the sampling distribution at each p value, you average all of 
          these prediction distributions together using the posterior probabilities of each p
          
          -> you get a posterior predictive distribution, that incorporates all of 
          uncertainty from the posterior distribution for the parameter p
                -> results in distribution of predictions being more honest, wider
                
            Calculation:
```{r}
#let's try p=0.6 and generate 10,000 simulated predictions for 9 tosses
w<-rbinom(1e4,size = 9, prob = 0.6) #predictions are stored as counts of water (so min is 0 and max is 9)

simplehist(w,xlab = "water count")

#to be sure that the samples are right.........
#define grid
p_grid<- seq(from=0, to=1, length.out=1000)

#define prior, flat prior in this case
prior<- rep(1,1000)

#compute likelihood at each value in grid
likelihood<- dbinom(6, size = 9, prob = p_grid)

#compute product of likelihood and prior
unstd.posterior<- likelihood*prior

#standardize the posterior, so it sums to 1
posterior<- unstd.posterior/sum(unstd.posterior)

#draw 10 000 samples from the posterior,(10 000= 1e4) read 1 times 10 to the power of 4 
samples<- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE) 
    #sample function randomly pulls values from the vector p_grid
    #the probability of each value is given by posterior 

#to propagate uncertainty: replace the parameter value with samples from the posterior
w<-rbinom(1e4,size = 9, prob = samples) #predictions are stored as counts of water (so min is 0 and max is 9)

simplehist(w,xlab = "water count")
```
            
                
          
          
          
Practice
    