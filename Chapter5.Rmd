---
title: "Chapter 5"
author: "Anita Kurm"
date: "March 29, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#wd
setwd("C:/Users/JARVIS/Desktop/Uni/Semester 4/Computational modelling/Statistical-Rethinking")

#libraries
pacman::p_load(readr,groupdata2,ggplot2,tidyverse,data.table, rethinking)

```

Multivariate Linear Models
  Multivariate regression - using more than one predictor variable to model an outcome
  Correlation is not rare, but it does not indicate causal relationship -> we need tools for distinguishing mere association from evidence of causation -> we do Multivariate regression, using more than one predictor variable to model an outcome
  Reasons:
  1) statistical "control" for confounds (variables that vary (change) with an independent variable,i.e. correlates with it). Simpson's paradox - the entire direction af an association between a predictor and otcome can be reversed byconsidering a confound. 
  2) Multiple causation. A phenomenon may really arise from multiple causes - measuring each cause is useful (they might hide though, multivariate models can help with that)
  3) Interactions.  The importance of each varibale may depend upon the other (even if they are uncorrelated). Interactions like combinations of variables (e.g. water+light are benefit for plant only together) occur very often --> Effective inference about one variable depends upon consideration of others

We focus on 1 and : we'll use multivariate regression to deal with simple confounds and to take multiple measurements of influence.

How to include arbitrary umber of main effects in your linear model of the Gaussian mean


Also dangers: multicollinearity

Categorical variables -> must be broken down into multiple predictor variables

5.1. Spurious association 
  The linear regression model for the divorce and the meadian age at marriage:
    Di ~ Normal(mui,sigma)       [likelihood]  Di - the divorce rate for State i
    mui = alpha + betaA *Ai      [linear model] Ai - State i's median age at mariage
    alpha ~ Normal(10,10)      [alpha prior]
    beta ~ Normal(0,1)          [beta prior]
    sigma ~ Uniform(0,10)        [sigma prior] 
```{r}
#load data
data(WaffleDivorce)
d<- WaffleDivorce

#standardize predictor - good habit
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage-mean(d$MedianAgeMarriage))/sd(d$MedianAgeMarriage)

#fit model
m5.1<- map(
  alist(
    Divorce~dnorm(mu,sigma),
    mu<- a + bA *MedianAgeMarriage.s, #linear model
    a ~ dnorm(10,10), #starts at the overall mean
    bA ~ dnorm(0,1),
    sigma~dunif(0,10)
  ),
  data = d)



#compute percentile interval of mean
#define sequence of weights to compute predictions for! 
#these values will be on the horizontal axis
MAM.seq <- seq(from=-3, to = 3.5, length.out = 30) 

#use link to compute mu
#for each sample from posterior
#and for each weight in weight.seq
mu <- link(m5.1, data = data.frame(MedianAgeMarriage.s=MAM.seq))
mu.PI <- apply(mu, 2, PI)
mu.mean <- apply(mu, 2, mean)

#visualize it
#show the first 100 values in the distribution of mu at each weight value  
#use type="n" to hide raw data
plot(Divorce ~ MedianAgeMarriage.s, data=d, col= rangi2)
#plot the MAP line, aka the mean mu for each median age marriage
lines(MAM.seq, mu.mean)
#plot a shaded region for 89%  PI
shade(mu.PI, MAM.seq)
```
    
  ispect precis:
```{r}
precis(m5.1, corr = TRUE)
```

  every std dev of delay in marriage (1.45 years  - sigma) predicts a decrease of about (bA) 1.04 divorce per   thousand adults, with a 89% interval from about -1.37 to -0.72.
  
  Try  the reproduce the plot showing that there's more divorces, if there is more marriages
  - duh...
```{r}
#standardize predictor - good habit
d$Marriage.s <- (d$Marriage-mean(d$Marriage))/sd(d$Marriage)

#fit model
m5.2<- map(
  alist(
    Divorce~dnorm(mu,sigma),
    mu<- a + bR *Marriage.s, #linear model
    a ~ dnorm(10,10), #starts at the overall mean
    bR ~ dnorm(0,1),
    sigma~dunif(0,10)
  ),
  data = d)



#compute percentile interval of mean
#define sequence of weights to compute predictions for! 
#these values will be on the horizontal axis
M.seq <- seq(from=-3, to = 3.5, length.out = 30) 

#use link to compute mu
#for each sample from posterior
#and for each weight in weight.seq
mu <- link(m5.2, data = data.frame(Marriage.s=M.seq))
mu.PI <- apply(mu, 2, PI)
mu.mean <- apply(mu, 2, mean)

#visualize it
#show the first 100 values in the distribution of mu at each weight value  
#use type="n" to hide raw data
plot(Divorce ~ Marriage.s, data=d, col= rangi2)
#plot the MAP line, aka the mean mu for each median age marriage
lines(M.seq, mu.mean)
#plot a shaded region for 89%  PI
shade(mu.PI, M.seq)


```
  plus precis thingie
```{r}
precis(m5.2,corr = TRUE)
```
  This shows an increase of 0.64 divorces for every additional standard deviation of marriage rate (1.67 - sigma) -- this relationship isnt as strong as the previous one.... i dunno, it says sigma 3.8 in the book
  
  Comparing parameter means between different bivariate regressions is not a way to decide which predictor is better
    both predictors could provide independent value
    they could be redundant
    one could eliminate the value of the other
    
    -> build the multivariate model to measur epartial value of each predictor
    it will answer questions:
      1) after i already know marriagerate, what additional value is there in also knowing         age at marriage
      2) After I already know age at marriage, what additional value is there in also knowing
      marriage rate
    parameter estimates to each predictor are the (often not transparent) answers
    
      5.1.1. Multivariate notation
      multivariate models add more parameters and variables to the definition of mui
      The strategy:
        1) nominate the predictor variables you want in the linear model of the mean
        2) for each predictor, make a parameter that will measure its association with the outcome 
        3) multiply the parameter by the variable and add that term to the linear model
        
      e.g.this model that predicts divorce rate, using both marriage rate and age at marriage
      Di ~ Normal(mui,sigma)          [likelihood]  Di - the divorce rate for State i
      mui = alpha + betaR*Ri+betaA*Ai [linear model] Ai - State i's median age at mariage
      alpha ~ Normal(10,10)           [alpha prior]
      betaR ~ Normal(0,1)             [betaR prior]     
      betaA ~ Normal(0,1)             [betaA prior]
      sigma ~ Uniform(0,10)           [sigma prior]
      
      R stands for rate, A stands for Age
      
      Assumption: mui = alpha + betaR*Ri+betaA*Ai means:
      the expected outcome for any State with marriage rate Ri and median age at marriage Ai
      is the sum of three independent terms:
      1) a constant: aplha. Every State gets this
      2) the product of the marriage rate, Ri, and the coefficient betaR - it measures the
      association between marriage rate and divorce rate
      3) similar, but it measures the association between age at marriage and divorce rate
      
      + reads like "or", indicates independent associations, which may be purely statistical or rather causal
      
      The linear model reads like: "A State's divorce rate can be a function of its marriage rate OR its median age at marriage"
      
      5.1.2. Fitting the model
      just expand the linear model..
      Di ~ Normal(mui,sigma)          [likelihood]  Di - the divorce rate for State i
      mui = alpha + betaR*Ri+betaA*Ai [linear model] Ai - State i's median age at mariage
      alpha ~ Normal(10,10)           [alpha prior]
      betaR ~ Normal(0,1)             [betaR prior]     
      betaA ~ Normal(0,1)             [betaA prior]
      sigma ~ Uniform(0,10)           [sigma prior]
```{r}
#fit the model
m5.3<- map(
  alist(
    Divorce~dnorm(mu,sigma),
    mu<- a + bR *Marriage.s+ bA*MedianAgeMarriage.s, #linear model
    a ~ dnorm(10,10),
    bR ~ dnorm(0,1), 
    bA ~ dnorm(0,1), 
    sigma~dunif(0,10)
  ),
  data = d)

precis(m5.3)

```
      bR: the posterior mean for marriage rate is now close to zero, with plenty of probability of both sides of zero
      bA: the posterior mean for age at marriage has gotten slightly farther from zero, but is essentially unchanged 
      
      visualize the posterior distribution estimates:
```{r}
plot(precis(m5.3))
```
      MAP values shown by the points, the percentile intervals by the horizontal lines
      Interpretation: Once we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State
      
      How did the model achieve that result??
      
      5.1.3. Plotting multivariate posteriors
      you need more plots!
      approach: learn to compute whatever you need from the model
      three types of interpretive plots:
        1)Predictor residual plots: show the outcome against residual predictor values
        2)Counterfactual plots: show the implied predictions for imaginary experiments in which the different predictor variables can be changed independently of one another
        3)Posterior prediction plots: show model-based predictions against raw data, or otherwise display the error in the prediction
        
          5.1.3.1. Predictor residual plots
          a predictor variable residual - the average prediction error when we use all of the other predictor variables to model a predictor of interest. <- does not make sense yet..
          benefits of computing it: once plotted against the outcome, we have a bivariate regression that has already "controlled" for all of the other predictor variables
            i.e. it just leaves in the variation that is not expected by the model of mu (the mean), as a function of the other predictors
          
          Example will make it clearer, i hope..
            Multivariate divorce rate model: two predictors
              1) marriage rate (R)
              2) median age at marriage (A)
              
              to compute predictor residuals for either, use the other one to model it
              So, for computing predictor residuals for marriage rate, we use age to model it
              
              Ri ~ Normal(mui,sigma)          [likelihood] Ri - the marriage rate for State i
              mui = alpha +beta*Ai      [linear model] Ai - State i's median age at mariage
              alpha ~ Normal(0,10)            [alpha prior]
              beta ~ Normal(0,1)             [beta prior]     
              sigma ~ Uniform(0,10) 
      
          we standardized both  R and A variables before -> we expect the mean alpha to be around zero -> so center  alpha's prior       
```{r}
#fit the model to get predictor residuals
m5.4<- map(
  alist(
    Marriage.s~dnorm(mu,sigma),
    mu<- a + b*MedianAgeMarriage.s, #linear model
    a ~ dnorm(0,10), #centered alpha
    b ~ dnorm(0,1), 
    sigma~dunif(0,10)
  ),
  data = d)

#compute the residuals:

#compute expected value at MAP, for each State (compute mu according to the linear model)
mu <- coef(m5.4)['a']+ coef(m5.4)['b']*d$MedianAgeMarriage.s

#compute residual for each State:  (the observed marriage rate in each State) -(the predicted marriage rate, based upon using age)
m.resid <- d$Marriage.s - mu
  ## if it's positive: the observed rate was in excess of what we'd expect, i.e. they marry fast for their age of marriage
  ## if it's negative: the observed rate was below what we'd expect, i.e. they marry slow for their age at marriage

#plot the relationship between these two variables, show the residuals as well
plot(Marriage.s ~ MedianAgeMarriage.s, d, col= rangi2)
abline(m5.4)
#loop over States
for (i in 1:length(m.resid)) {
  x <- d$MedianAgeMarriage.s[i] #location of line segment
  y <- d$Marriage.s[i] #observed endpoint pf line segment
  #draw the line segment
  lines(c(x,x), c(mu[i],y), lwd=0.5, col=col.alpha("black",0.7))
}

```
        Residual marriage rate in each State, after accounting for the linear association with median age at marriage.
        Each gray line segment is residual - the distance of each observed marriage rate from the expected value - attempting to predict marriage rate with median age at marriage
        Those residuals below the line have lower actual marriage rates than it was predicted by using the age 
        The residuals are variation in marriage rate that is left over, after taking out the purely linear relationship between the two variables 
        
        Now, we calculated residuals, how do we use them?
        Put them on a horizontal axis and plot them against the actual otcome of interest, i.e. divorce rate
```{r}
#fit the model to get predictor residuals
m5.4<- map(
  alist(
    Marriage.s~dnorm(mu,sigma),
    mu<- a + b*MedianAgeMarriage.s, #linear model
    a ~ dnorm(0,10), #centered alpha
    b ~ dnorm(0,1), 
    sigma~dunif(0,10)
  ),
  data = d)

#compute the residuals:

#compute expected value at MAP, for each State (compute mu according to the linear model)
mu <- coef(m5.4)['a']+ coef(m5.4)['b']*d$MedianAgeMarriage.s

#compute residual for each State:  (the observed marriage rate in each State) -(the predicted marriage rate, based upon using age)
m.residRate <- d$Marriage.s - mu
  ## if it's positive: the observed rate was in excess of what we'd expect, i.e. they marry fast for their age of marriage
  ## if it's negative: the observed rate was below what we'd expect, i.e. they marry slow for their age at marriage

#plot the relationship between these two variables, show the residuals as well
plot(Marriage.s ~ MedianAgeMarriage.s, d, col= rangi2)
abline(m5.4)
#loop over States
for (i in 1:length(m.residRate)) {
  x <- d$MedianAgeMarriage.s[i] #location of line segment
  y <- d$Marriage.s[i] #observed endpoint pf line segment
  #draw the line segment
  lines(c(x,x), c(mu[i],y), lwd=0.5, col=col.alpha("black",0.7))
}

#standardize predictor - good habit
d$Divorce.s <- (d$Divorce-mean(d$Divorce))/sd(d$Divorce)

#fit the model? divorce rate and m.resid
m5.5<- map(
  alist(
    Divorce~dnorm(mu,sigma),
    mu<- a + b*m.residRate, #linear model
    a ~ dnorm(0,10), #centered alpha
    b ~ dnorm(0,1), 
    sigma~dunif(0,10)
  ),
  data = d)

#compute percentile interval of mean
#define sequence of weights to compute predictions for! 
#these values will be on the horizontal axis
M.seq <- seq(from=-3, to = 3.5, length.out = 30) 

#use link to compute mu
#for each sample from posterior
#and for each weight in weight.seq
mu <- link(m5.5, data = data.frame(m.residRate=M.seq))
mu.PI <- apply(mu, 2, PI)
mu.mean <- apply(mu, 2, mean)

#plot
plot(Divorce ~ m.residRate, d, col= rangi2)
abline(m5.5)
#plot a shaded region for 89%  PI
shade(mu.PI, M.seq)
mtext("About the same divorce rates in states with fast and slow marriage rates for their median age of marriage")

######################### Median age at marriage "controlling" for marriage rate

#get the predictor residuals for age by using rates 
#fit the model to get predictor residuals
m5.6<- map(
  alist(
    MedianAgeMarriage.s~dnorm(mu,sigma),
    mu<- a + b*Marriage.s, #linear model
    a ~ dnorm(0,10), #centered alpha
    b ~ dnorm(0,1), 
    sigma~dunif(0,10)
  ),
  data = d)

#compute the residuals:

#compute expected value at MAP, for each State (compute mu according to the linear model)
mu <- coef(m5.6)['a']+ coef(m5.6)['b']*d$Marriage.s

#compute residual for each State:  (the observed marriage rate in each State) -(the predicted marriage rate, based upon using age)
m.residAge <- d$MedianAgeMarriage.s - mu
  ## if it's positive: the observed rate was in excess of what we'd expect, i.e. they marry fast for their age of marriage
  ## if it's negative: the observed rate was below what we'd expect, i.e. they marry slow for their age at marriage

#plot the relationship between these two variables, show the residuals as well
plot(MedianAgeMarriage.s ~ Marriage.s, d, col= rangi2)
abline(m5.6)
#loop over States
for (i in 1:length(m.residAge)) {
  x <- d$Marriage.s[i] #location of line segment
  y <- d$MedianAgeMarriage.s[i] #observed endpoint pf line segment
  #draw the line segment
  lines(c(x,x), c(mu[i],y), lwd=0.5, col=col.alpha("black",0.7))
}


#fit the model. divorce rate and m.resid for age "controlling" for rate
m5.7<- map(
  alist(
    Divorce~dnorm(mu,sigma),
    mu<- a + b*m.residAge, #linear model
    a ~ dnorm(0,10), #centered alpha
    b ~ dnorm(0,1), 
    sigma~dunif(0,10)
  ),
  data = d)

#compute percentile interval of mean
#define sequence of weights to compute predictions for! 
#these values will be on the horizontal axis
M.seq <- seq(from=-3, to = 3.5, length.out = 30) 

#use link to compute mu
#for each sample from posterior
#and for each weight in weight.seq
mu <- link(m5.7, data = data.frame(m.residAge=M.seq))
mu.PI <- apply(mu, 2, PI)
mu.mean <- apply(mu, 2, mean)

#plot
plot(Divorce ~ m.residAge, d, col= rangi2)
abline(m5.7)
#plot a shaded region for 89%  PI
shade(mu.PI, M.seq)
mtext("Lower divorce rates for States with old median age of marriage for their marriage rate")
```
        At 0 on x-axis is the border between  slower and faster than expected for the age rates (plot 1) and younger and older  than expected for the rates median age (plot 2)
        The slopes of the regression line of model 5.5 and 5.7 are -0.13 and -1.13, just as in the model 5.3 
```{r}
precis(m5.3)
precis(m5.5)
precis(m5.7)

```
        ... smth weird happened with beta estimation in m5.5 (it's supposed to be the same as betaR in m5.3)
        
        There's direct value in seeing the model-based predictions displayed against the outcome, after subtracting out the influence of other predictors
        The plots above do this.
        
        Linear regression models do all of this with a very specific additive model of how rge predictors relate to one another
        
        but but but there might be non-additive ways of predictors being related!!
        
        the logic is the same, but details are very different..
        so we need smth more general..
        
    
      5.1.3.2 Counterfactual plots
      inferential plot, that displays the implied predictions of the model
      "counterfactual" cause can be produced far any values of the predictor variables you like, even if the combination is not possible in real life, or not observed in data
          e.g. a state with very high marriage rate and very high median age at marriage
          
        The simplest use: see how the predictions change, as you change only one predictor at a time, i.e. holding the values of all predictors constant, except for a single predictor of interest
          ...will not necessarily look like your raw data (cause counterfactual, duh), but will help to understand the implications of the model
          these plots are priceless!!
          
```{r}
#just to see the model, that we want to know implications of
m5.3<- map(
  alist(
    Divorce~dnorm(mu,sigma),
    mu<- a + bR *Marriage.s+ bA*MedianAgeMarriage.s, #linear model
    a ~ dnorm(10,10),
    bR ~ dnorm(0,1), 
    bA ~ dnorm(0,1), 
    sigma~dunif(0,10)
  ),
  data = d)

#prepare new counterfactual data
A.avg <- mean(d$MedianAgeMarriage.s)
R.seq <- seq(from=-3, to=3, length.out = 30)
pred.data <- data.frame(
  Marriage.s=R.seq,
  MedianAgeMarriage.s=A.avg
)

#compute counterfactual mean divorce (mu)
mu <- link(m5.3, data = pred.data)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

#simulate counterfactual divorce outcomes
R.sim <- sim(m5.3, data=pred.data, n=1e4)

#compute percentile intervals of simulated divorce outcomes
R.PI <- apply(R.sim, 2, PI)

#dispay predictions, hiding raw data with type="n"
plot(Divorce ~ Marriage.s, data=d, type="n")
mtext("MedianAgeMarriage.s=0")
lines(R.seq, mu.mean)
shade(mu.PI, R.seq)
shade(R.PI, R.seq)

```
          Counterfactual plot for the multivariate divorce model,m5.3. The plot shows the change in predicted divorce mean across values of a marriage rate, holding the other predictor, age at marriage, constant at its mean value (zero). 
          89% percentile intervals of the mean (dark, narrow)
          89% prediction intervals (light, wide)
          
        Now do the same, but Marriage.s is set to its average and MedianAgeMarriage.s allowed to vary
```{r}
#prepare new counterfactual data, where Marriage rate is set to its average and it does not change
R.avg <- mean(d$Marriage.s)
A.seq <- seq(from=-3, to=3, length.out = 30)
pred.data2 <- data.frame(
  Marriage.s=R.avg,   #avg predictor is held constant at its mean 
  MedianAgeMarriage.s=A.seq #the seq predictor changes across the values in the defined sequence
)

#compute counterfactual mean divorce (mu)
mu <- link(m5.3, data = pred.data2)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

#simulate counterfactual divorce outcomes
A.sim <- sim(m5.3, data=pred.data2, n=1e4)

#compute percentile intervals of simulated divorce outcomes
A.PI <- apply(A.sim, 2, PI)

#dispay predictions, hiding raw data with type="n"
plot(Divorce ~ MedianAgeMarriage.s, data=d, type="n")
mtext("Marriage.s=0")
lines(A.seq, mu.mean)
shade(mu.PI, A.seq)
shade(A.PI, A.seq)
```
        Counterfactual plot for the multivariate divorce model,m5.3. The plot shows the change in predicted divorce mean across values of age at marriage, holding the other predictor, marriage rate, constant at its mean value (zero). 
          89% percentile intervals of the mean of divorce (dark, narrow)
          89% prediction intervals (light, wide)
          
          
          Both plots above don't display any data, since they are counterfactual
          They have the same slopes as the residual plots before them
          They also show percentile intervals on the scale of the data (not weird residual scale)
          -> They are direct displays of the impact on prediction of a change in each varibale
          
          These counterfactual plots always help in understanding the model, they may also mislead by displaying predictions for impossible combinations of predictor values 
          
          
          5.1.3.3. Posterior prediction plots 
          It's important to understand the estimates, but it's also important to check the model fit against the observed data
          
          Chapter 3: simulate globe tosses, averaging over the posterior, and compared the simulated results to the observed
          
          Two uses of these checks:
            1) Did the model fit correctly? Models make mistakes, as the creators of the models (common software and user errors). The most common errors can be easily diagnosed by comparing implied predictions to raw data
            2) How does the model fail? Models are useful fictions, they always fail in some way. Sometimes the model fits correctly, but it must be discarded cause it's so poor for our purposes. Often, a model predicts well in some respects, but not the others. Inspect the individual cases where the model makes poor predictions to get an idea of how to improve the model.
            
```{r}
###########simulate predictions, averaging over the posterior#################

#call link without specifying new data, so it uses original data to find the mean of divorce rates
mu <- link(m5.3)

#summarize samples across cases
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

#simulate divorce rate observations
#no new data, so it uses original data
divorce.sim <- sim(m5.3, n=1e4)

#compute percentile intervals of simulated divorce rate observations
divorce.PI <- apply(divorce.sim, 2, PI)

#plot the simulated observations (predictions) against observed
plot(mu.mean ~ d$Divorce, col=rangi2, ylim=range(mu.PI),
     xlab="Observed divorce rate", ylab="Predicted divorce rate")
#add a line to show perfect prediction 
abline(a=0, b=1, lty=2)
#add line segments for the confidence interval of each prediction
#loop over all cases 
for (i in 1:nrow(d))
  #draw the line segment
  lines(rep(d$Divorce[i],2), c(mu.PI[1,i],mu.PI[2,i]),
        col=rangi2)

#label a few select points
identify(x=d$Divorce, y = mu.mean, labels = d$Loc, cex=0.8)    #does not work the way it does in the book...

```
          The model under-predicts for States with very high divorce rates (e.g. an actual rate of over 12, the model predicted to be under 10 ), 
          while it over-predicts for States with very low divorce rates 
          
          The further the points are from the perfect prediction line (observed = predicted), the more frustrating they are to the model 
          It's hard to see the amount of prediction error, so we also use residual plots, that show the mean prediction error for each row + we can sort the states from lowest prediction error to highest
          
```{r}
#compute residuals
divorce.resid <- d$Divorce - mu.mean

#get ordering by divorce residuals
o <- order(divorce.resid)

#make the plot
dotchart(divorce.resid[o], labels = d$Loc[o], xlim=c(-6,5), cex=0.6)
abline(v=0, col=col.alpha("black", 0.2))
#loop over all cases 
for (i in 1:nrow(d)) {
  j<- o[i] #which state in order
  #draw the line segments
  lines(d$Divorce[j] - c(mu.PI[1,j], mu.PI[2,j]), rep(i,2,))
  points(d$Divorce[j] - c(divorce.PI[1,j], divorce.PI[2,j]), rep(i,2,), 
         pch=3, cex=0.6, col="gray")
}
```
        Here you can easily see the large model failures, such as Idaho (ID) and Maine (ME)  
          better try with ggplot later though...
          These plots take up a lot of space, if you have many rows in your data.
          
        Another way to use these simulations:
          Construct novel predictor residual plots:
            you compute the divorce residuals, and plot them against new predictor variables
            -> you can see if remaining variation in the outcome is associated with another predictor
             e.g.
```{r}
#plot the divorce residuals against Waffle Houses per capita
d$WafflePerCapita <- d$WaffleHouses/d$Population


#these values will be on the horizontal axis
M.seq <- seq(from=0, to = 40, length.out = 30) 


m5.8<- map(
  alist(
    divorce.resid~dnorm(mu,sigma),
    mu<- a + b *WafflePerCapita, #linear model
    a ~ dnorm(10,10),
    b ~ dnorm(0,1), 
    sigma~dunif(0,10)
  ),
  data = d)
#use link to compute mu
#for each sample from posterior
#and for each weight in weight.seq
mu <- link(m5.8,data = data.frame(WafflePerCapita=M.seq))
mu.PI <- apply(mu, 2, PI)
mu.mean <- apply(mu, 2, mean)


#plot the simulated observations (predictions) against observed
plot(divorce.resid ~ d$WafflePerCapita, col=rangi2,
     xlab="Waffle houses per capita", ylab="Divorce rate prediction error")
abline(m5.8)
#plot a shaded region for 89%  PI
shade(mu.PI, M.seq)


```
             
      the line showing 89% interval of the mean, and 89% prediction interval (gray shaded area) 
      A small positive correlation remains between divorce rate and Waffle House, despite already controlling for marriage rate and age at maarriage ----- this does not mean that the correlation is real, there basically always will be possible to find spurious correlations with remaining correlation, after controlling for all meaningful predictors you included before..
  
  
------------------------------------------------------------------------------------  
      

  5.2. Masked relationship
  Multiple predictor variables are useful for knocking out spurious association
  
  Another reason to use more than one predictor: to measure the direct influences of multiple factors on an outcome, when none of those influences is apparent from bivariate relationships
        this kind of problem: two predictor variables are correlated with one another, but one of
        them is positively correlated with the outcome and the other is negatively correlated with it
  
  the milk data 
```{r}
library(rethinking)
data(milk)
d<- milk
str(d)
```
  A popular hypothesis: the primates with larger brains produce more energetic milk, so that brains can grow quickly
  data variables:
  kcal.per.g: Kilocalories of energy per gram of milk
  mass: Average female body mass, in kg
  neocortex.perc: The percent of total brain mass that is neocortex mass
  
  Question: to what extent energy content of milk is related to the neocortex percent of the brain
    We'll need female body mass to see the masking that hides the relationships among the variables
    
  The simple bivariate regression between kcal and neocortex %
```{r}
m5.5 <- map(
  alist(
    kcal.per.g~dnorm(mu,sigma),
    mu<- a + bn *neocortex.perc, #linear model
    a ~ dnorm(0,100),
    bn ~ dnorm(0,1), 
    sigma~dunif(0,1)
  ),
  data = d)
```
  the error cause the missing values in neocortex.perc
  fixed:
```{r}
dcc <- d[complete.cases(d),]
```
  just 17 observations, it dropped 12. It's good if we know that we drop smth, so more automated models like glm are dangerous in that sense

```{r}
#fit the model on new dataframe
m5.5 <- map(
  alist(
    kcal.per.g~dnorm(mu,sigma),
    mu<- a + bn *neocortex.perc, #linear model
    a ~ dnorm(0,100),
    bn ~ dnorm(0,1), 
    sigma~dunif(0,1)
  ),
  data = dcc)

#take a look at the quadratic approximate posterior
precis(m5.5, digits = 3)
```
  the posterior for beta n is very small, so we added digits=3 to see that it's not exactly zero
  A change from the smallest neocortex percent (55%) in data to the largest (76%) would result in an expected change only:
```{r}
coef(m5.5)["bn"]*(76-55)
```
  less than 0.1 kcal. The kcal in the data range from less than 0.5 to more than 0.9 per gram, so this association is not so impressive
  it's also not precise, the 89% interval of parameter extends a good distance of both sides of zero
```{r}
#fit the model on new dataframe
m5.5 <- map(
  alist(
    kcal.per.g~dnorm(mu,sigma),
    mu<- a + bn *neocortex.perc, #linear model
    a ~ dnorm(0,100),
    bn ~ dnorm(0,1), 
    sigma~dunif(0,1)
  ),
  data = dcc)

#seq and pred.data for the predictor
np.seq <- 0:100
pred.data <- data.frame(neocortex.perc=np.seq)

mu <- link(m5.5, data=pred.data, n=1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

#plot
plot(kcal.per.g ~ neocortex.perc, data=dcc, col=rangi2,
     xlab="Neocortex percent", ylab="Energy content of milk")
abline(m5.5)
#plot a shaded region for 89%  PI
shade(mu.PI, np.seq)

#plot2, same but different
plot(kcal.per.g ~ neocortex.perc, data=dcc, col=rangi2,
     xlab="Neocortex percent", ylab="Energy content of milk")
lines(np.seq,mu.mean)
lines(np.seq, mu.PI[1,], lty=2)
lines(np.seq, mu.PI[2,], lty=2)

```
  The MAP line wis weakly positive, but it's highly imprecise.
    i.e. a lot of mildly positive and negative slopes are plausible, given this model and these data
  
  Now consider mass as a predictor, take log transofrmed measurements
    Scaling measurements like body mass are related by magnitudes to other variables
    Taking the log of a measure translates the measure into magnitudes
    
    here: We suspect that the magnitude of a mother's body is related to milk energy, in a linear fashion -> we use the logarithm of body mass
```{r}
#create a new column
dcc$log.mass <- log(dcc$mass)

#fit the model
m5.6 <- map(
  alist(
    kcal.per.g~dnorm(mu,sigma),
    mu<- a + bm *log.mass, #linear model
    a ~ dnorm(0,100),
    bm ~ dnorm(0,1), 
    sigma~dunif(0,1)
  ),
  data = dcc)

precis(m5.6)

#seq and pred.data for the predictor
lm.seq <- -30:100
pred.data <- data.frame(log.mass=lm.seq)

mu <- link(m5.6, data=pred.data, n=1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

#plot
plot(kcal.per.g ~ log.mass, data=dcc, col=rangi2,
     xlab="Log mass", ylab="Energy content of milk")
abline(m5.6)
#plot a shaded region for 89%  PI
shade(mu.PI, lm.seq)

#plot2, same but different
plot(kcal.per.g ~ log.mass, data=dcc, col=rangi2,
     xlab="Log mass", ylab="Energy content of milk")
lines(lm.seq,mu.mean)
lines(lm.seq, mu.PI[1,], lty=2)
lines(lm.seq, mu.PI[2,], lty=2)
```
   Log mass is negatively correlated with kilocalories, the influence seem stronger than that of neocortex percent, but in the opposite direction <- quite uncertain though
  
  But what happens when we add both predictors at the same time to the regression
    The math form of the multivariate model:

      ki ~ Normal(mui,sigma)          [likelihood] ki - the kcal.per.g for species i
      mui = alpha +beta n *ni +beta m *log(mi) [linear model] ni - Species i's neocortex %, mi - mass
      alpha ~ Normal(0,100)            [alpha prior]
      beta n ~ Normal(0,1)             [beta prior for neocortex% predictor]
      beta m ~ Normal (0,1)            [beta prior for mass predictor]
      sigma ~ Uniform(0,10)            [std dev prior]
      
```{r}
#fit the model
m5.7 <- map(
  alist(
    kcal.per.g~dnorm(mu,sigma),
    mu<- a +bn*neocortex.perc+ bm *log.mass, #linear model
    a ~ dnorm(0,100),
    bn ~ dnorm(0,1),
    bm ~ dnorm(0,1), 
    sigma~dunif(0,1)
  ),
  data = dcc)

precis(m5.7)
```
   Inocorporating both predictor variables in the regression increased the estimated association of both with the outcome    
   bn <- the posterior mean for the association of neocortex % has increased more than sixfold, its 89% interval is now entirely above 0
   bm <- the posterior mean for the association of log body mass is more strongly negative
   
   Counterfactual plots (using mean log body mass) showing only how predicted energy varies as a function of  neocortex percent
```{r}
#fit the model
#m5.7 <- map(
#  alist(
#    kcal.per.g~dnorm(mu,sigma),
#    mu<- a +bn*neocortex.perc+ bm *log.mass, #linear model
#    a ~ dnorm(0,100),
#    bn ~ dnorm(0,1),
#    bm ~ dnorm(0,1), 
#    sigma~dunif(0,1)
#  ),
#  data = dcc)

#fixate the mean log mass to be a constant 
mean.log.mass <- mean(log (dcc$mass))

#sequence of possible values for neocortex %
np.seq<- 0:100
pred.data <-data.frame(
  neocortex.perc=np.seq,
  log.mass=mean.log.mass
)

mu <- link(m5.7, data=pred.data, n=1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

#plot
plot(kcal.per.g ~ neocortex.perc, data=dcc, type="n",
     xlab="Neocortex %", ylab="Energy content of milk")
mtext("Log body mass = constant average of log body mass")
lines(np.seq, mu.mean)
#plot a shaded region for 89%  PI
shade(mu.PI, np.seq)

#plot2, same but different, border lines instead of shaded area
plot(kcal.per.g ~ neocortex.perc, data=dcc, type="n",
     xlab="Neocortex %", ylab="Energy content of milk")
mtext("Log body mass = constant average of log body mass")
lines(np.seq,mu.mean)
lines(np.seq, mu.PI[1,], lty=2)
lines(np.seq, mu.PI[2,], lty=2)

```
   The counterfactual plot for the multivariate milk energy content model, m5.7. The plot shows the change in predicted values of mean energy content of milk across values of neocortex percentage, constantly holding the log body mass at its average (mean value, i.e. 1.499).
   
```{r}
#fit the model
#m5.7 <- map(
#  alist(
#    kcal.per.g~dnorm(mu,sigma),
#    mu<- a +bn*neocortex.perc+ bm *log.mass, #linear model
#    a ~ dnorm(0,100),
#    bn ~ dnorm(0,1),
#    bm ~ dnorm(0,1), 
#    sigma~dunif(0,1)
#  ),
#  data = dcc)

#fixate the mean neocortex perc to be a constant 
mean.neocortex.perc <- mean(dcc$neocortex.perc)

#sequence of possible values for the predictor on x axis (log body mass)
lm.seq<- seq(from=-4, to=4, length.out = 100)

#create dataframe for counterfactual plot: logmass is all possible log mass values on x axis, neocortex perc is constant, which is unrealistic
pred.data2 <-data.frame(
  neocortex.perc=mean.neocortex.perc,
  log.mass=lm.seq
)

mu <- link(m5.7, data=pred.data2, n=1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

#plot
plot(kcal.per.g ~ log.mass, data=dcc, type="n",
     xlab="Log body mass", ylab="Energy content of milk")
mtext("Neocortex % = constant average of neocortex %")
lines(lm.seq, mu.mean)
#plot a shaded region for 89%  PI
shade(mu.PI, lm.seq)

#plot2, same but different, border lines instead of shaded area
plot(kcal.per.g ~ log.mass, data=dcc, type="n",
     xlab="Log body mass", ylab="Energy content of milk")
mtext("Neocortex %= constant average of neocortex %")
lines(lm.seq,mu.mean)
lines(lm.seq, mu.PI[1,], lty=2)
lines(lm.seq, mu.PI[2,], lty=2)
```
   
  
  Adding neocortex and body mass to the same model led to larger estimated effects of both
  Why?
    two variables correlated with the outcome, but in different directions
    + both of the explanatory variables are positively correlated with one another -> they tend to cancel one another out
    <- another case in which regression automatically finds the most revealing cases and uses them to produce estimates
      e.g. The regression model asks if species that have high neocortex percent for their body mass have higher milk energy 
      
      
  5.3. When adding variables hurts
  why not just fit a model that includes all available predictors in the dataframe?
    Reasons:
    1) multicolinearity: very strong correlation between 2+ predictor variables
      -> posterior dstrb will show a very large range of plausible parameter values
    2) post-treatment bias: statistically controlling for consequences of a causal factor
    ->
    3) overfitting
    
    
    5.3.1. Multicollinear logs
      simulation: individual's height ~ the length of left leg + the length of right leg
```{r}
# number of individuals
N <- 100

#sim total height of each
height <- rnorm(N,10,2) #generate N normal random numbers with arguments mean and sd

#leg as proportion of height
leg_prop <- runif(N,0.4,0.5) #generate N uniform random numbers lie in the interval (min, max) <- individual's legs are 45% of the height (on average in these data)

#sim left leg as proportion + error
leg_left <- leg_prop*height + rnorm( N , 0 , 0.02 )

#sim right leg as proportion + error
leg_right <- leg_prop*height + rnorm( N , 0 , 0.02 )

#combine into data frame
d <- data.frame(height,leg_left,leg_right)

#fit a model
m5.8 <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + bl*leg_left + br*leg_right ,
    a ~ dnorm( 10 , 100 ) ,
    bl ~ dnorm( 2 , 10 ) , #beta measures association of a leg with height, should be around the avg height/ 45% of avg height (10/4.5 = 2.2ish)
    br ~ dnorm( 2 , 10 ) , 
    sigma ~ dunif( 0 , 10 )
    ) ,
  data=d )
precis(m5.8)
```
  Looks bad, man. +different from the book (cause simulation variance)
```{r}
plot(precis(m5.8))
```
  The model fit correct, but why does the posterior look so weird?   
      